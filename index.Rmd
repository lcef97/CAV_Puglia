---
title: "Exploratory analysis of accesses to support centers for gender-based violence
  in Apulia"
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(sf)
if(!rlang::is_installed("pscl")) install.packages("pscl")
```

```{r input, echo = FALSE, message = FALSE, warning = FALSE, output = FALSE}
load("input/CAV_input_mun_2022.RData")

#' 2022 municipalities shapefiles; easily obtainable by scraping with the following 
#' commented code:
#' Mun22_shp <- SchoolDataIT::Get_Shapefile(2022)
#' Shp <- Mun22_shp %>% dplyr::filter(.data$COD_REG == 16) %>% 
#'  dplyr::select(.data$COD_PROV, .data$PRO_COM, .data$COMUNE)
#'
#' Still, we leave the static shapefile in order NOT to need internet connection:
load("input/Shp.RData")

# Function to extract numeric digits from a strings vector (needed to filter age):
nn_extract <- function(string){
  nn <- gregexpr("([0-9])", string)
  ls.out <- regmatches(as.list(string), nn)
  res <- unlist(lapply(ls.out, function(x) as.numeric(paste(x, collapse = ""))))  
  return(res)
}

# Female population aged >= 15 years.
# Source data: http://dati.istat.it/Index.aspx?DataSetCode=DCIS_POPRES1#
Popolazione_Puglia_2022 <- readr::read_csv("input/Popolazione_Puglia_2022.csv") %>% 
  dplyr::select(.data$ITTER107, .data$Territorio, .data$SEXISTAT1, .data$ETA1, .data$Value) %>% 
  dplyr::filter(.data$ETA1 != "TOTAL") %>% 
  dplyr::mutate(ETA1 = nn_extract(ETA1)) %>% 
  dplyr::mutate(ITTER107 = as.numeric(ITTER107))
names(Popolazione_Puglia_2022) <- c("PRO_COM", "Comune", "Sesso", "Eta", "Popolazione")

# Filter and aggregate:
Pop_f_15 <- Popolazione_Puglia_2022 %>% dplyr::filter(.data$Sesso == 2) %>% 
  dplyr::filter(.data$Eta > 14) %>% 
  dplyr::group_by(.data$PRO_COM, .data$Comune) %>% 
  dplyr::summarise(nn = sum(.data$Popolazione)) %>% dplyr::ungroup()

# Complete dataset:
dd <- Shp %>% dplyr::left_join(Pop_f_15[,c(1,3)],
                               by = "PRO_COM") %>% 
  dplyr::left_join(dplyr::select(CAV_mun_22, -.data$comune),by = "PRO_COM") %>% 
  dplyr::left_join(dplyr::select(Indicators, -.data$Comune), by = "PRO_COM")

# Municipalities from which no woman reported violence --> count == 0
dd$N_ACC[is.na(dd$N_ACC)] <- 0 
# "access ratio"
dd$F_ACC <- dd$N_ACC/dd$nn

munWcav <- munWcav <- c (71020,71024,71051,72004,72006,72011,72014,
                         72019,72021,72029,72031,72033,72035,73013,
                         73027,74001,74009,75018,75029,75035,75059,
                         110001,110002,110009)

# Tremiti Islands are a singleton --> need to remove them
# in order to perform spatial analysis
suppressWarnings({
  singletons <- which(unlist(lapply(spdep::poly2nb(dd), function(x) x[1L] == 0)))
})


# Filter out singletons
dd_con <- dd[-singletons, ]

load("input/dists_th_22.RData")

# This is the dataset we will concretely work on:
dd_con <- dd_con %>% 
  dplyr::left_join(dists_th_22, by = "PRO_COM") %>% 
  dplyr::mutate(TEP_th_22 = as.vector(scale(.data$TEP_th_22)))  %>% 
  dplyr::mutate(AES = as.vector(scale(.data$AES))) %>% 
  dplyr::mutate(MFI = as.vector(scale(.data$MFI)))  %>% 
  dplyr::mutate(PDI = as.vector(scale(.data$PDI)))  %>% 
  dplyr::mutate(ELL = as.vector(scale(.data$ELL)))  %>% 
  dplyr::mutate(ER = as.vector(scale(.data$ER)))  %>% 
  dplyr::mutate(PGR = as.vector(scale(.data$PGR)))  %>% 
  dplyr::mutate(UIS = as.vector(scale(.data$UIS)))  %>% 
  dplyr::mutate(ELI = as.vector(scale(.data$ELI))) 

# sd of travel time: almost 16 minutes
#attr(scale(dists_th_22$TEP_th_22), "scaled:scale")

# neighbours list
nb_con <- spdep::poly2nb(dd_con)
# neighbouring/adjacency matrix
W_con <- spdep::nb2mat(nb_con, style = "B")
rownames(W_con) <- colnames(W_con) <- dd_con$PRO_COM
Lapl_con <- diag(rowSums(W_con)) - W_con
V_con <- eigen(Lapl_con)$vectors
# row ID - needed for spatial models
dd_con$ID <- c(1:nrow(dd_con))

# Full GLM --> for model matrix
glm_all_X <- glm(N_ACC ~ 1 + TEP_th_22 + MFI + AES + PDI + ELL + ER +
                   PGR + UIS + ELI + offset(log(nn)),
                 data = dd_con, family = "poisson")
# model matrix
X <- model.matrix(glm_all_X)


```


## Data
The dataset employed regards the counts of accesses to gender-based violence support centers in the Apulia region by residence municipality of the women victims of violence during 2022. `R` codes to generate the dataset are in the R script [posted here](https://github.com/lcef97/CAV_Puglia/blob/main/CAV_2022.R) which this report is based on.

Here, we only take into account the violence reports which support centers actually take charge of, at the risk of underestimating the counts of gender-based violence cases.
This choice is driven by the need of avoiding duplicated records, since e.g. it may happen that a support center redirects a victim to another support center. 

In order to avoid singletons in the spatial structure of the dataset, we removed the Tremiti Islands from the list of municipalities included ($0$ accesses to support centers in 2022).

Therefore, the municipality-level dataset in scope consists of $256$ observations. 

We can only take into account the accesses to support centers for which the origin municipality of victims is reported. Therefore, the total count of accesses in scope is $2259$. Among these accesses, $1516$ were taken charge of. 

Here, we plot the log-access rate per residence municipality, i.e. the logarithm of the ratio between access counts and female population. Blank areas correspond to municipalities from which zero women accessed support centers ($82$ municipalities).


```{r Log accesses plot, echo = FALSE, warning = FALSE, message = FALSE, fig.height = 3}
dd_con %>% 
  dplyr::mutate(rate = log(.data$N_ACC) - log(.data$nn)) %>% 
  ggplot2::ggplot() +
  ggplot2::geom_sf(ggplot2::aes(fill = .data$rate))+
  ggplot2::labs("Log incidence ratio") +
  ggplot2::scale_fill_viridis_c(na.value = "white", direction = -1) +
  ggplot2::theme_classic()
  

```

## Covariates

Our target is explaining the number of accesses to support centers, $y$, defined at the municipality level, on the basis of a set of candidate known variables.

We model $y$ via a simple Poisson GLM.

We have at disposal a number of candidate explanatory variables, which include the distance of a municipality from the closest support center and a set of variables measuring social vulnerability under different dimensions; these latter covariates are provided by the ISTAT.A more detailed description of these covariates is in [this excel metadata file](https://github.com/lcef97/CAV_Puglia/blob/main/Metadata/Indice_composito_fragilita_PUGLIA_2021.xlsx).

All covariates are scaled to have null mean and unit variance.

<!-- :::{.only-html} -->
  -  $\mathrm{TEP}$, i.e. the distance of each municipality from the closest municipality hosting a support center. Distance is measured by road travel time in minutes (acronym TEP stays for Tempo Effettivo di Percorrenza, i.e. Actual Travel Time).
  
For instance, the support center designated for the municipality of Adelfia (province of Bari, $3$rd municipality in the dataset) is located in Capurso (BA). Then, $\mathrm{TEP}_{3}$ denotes the travel time between Adelfia and Capurso ($17$ minutes). 

  -  $\mathrm{AES}$, the distance from the closest infrastructural pole, always measured in travel time.
  -  $\mathrm{MFI}$, i.e. the decile of municipality vulnerability index.
  -  $\mathrm{PDI}$, i.e. the dependency index, i.e. population either $\leq 20$ or $\geq 65$ years over population in $[20 - 64]$ years.
  -  $\mathrm{ELL}$, i.e. the proportion of people aged $[25-54]$ with low education.
  -  $\mathrm{ERR}$, i.e. employment rate among people aged $[20-64]$.
  -  $\mathrm{PGR}$, i.e. population growth rate with respect to 2011.
  -  $\mathrm{UIS}$, i.e. the ventile of the density of local units of industry and services (where density is defined as the ratio between the counts of industrial units and population).
  -  $\mathrm{ELI}$, i.e. the ventile of employees in low productivity local units by sector for industry and services.

<!--  ::: -->




First, we visualise the correlations among these explanatory variables:


```{r correls, echo = F, warning = F, fig.height = 3}
ggplot2::ggplot(data = reshape2::melt(cor(X[,-1]))) +
  ggplot2::geom_tile(ggplot2::aes(
    x = .data$Var2, y = .data$Var1, fill = .data$value), color = "black", size = 3) +
  ggplot2::geom_text(ggplot2::aes(
    x = .data$Var2, y = .data$Var1, label = round(.data$value, 2))) +
  ggplot2::scale_fill_gradient2(
    low = "blue", mid = "white", high = "red", midpoint = 0) +
  ggplot2::theme_minimal()
```

We see the correlation between the two distances is very high ($0.72$), and so is the correlation between the fragility index decile and the density of productive units. 

In the first case, we drop the distance from the nearest infrastructural pole We do so because, if taken alone, the distance from the closest support center appears a slightly better predictor, using the Schwarz information criterion (or, indifferently, the Akaike Information Criterion):
```{r BICS single covar}

stats::BIC(glm(N_ACC ~ 1 + AES, family = "poisson",
               offset = log(nn), data = dd_con))

stats::BIC(glm(N_ACC ~ 1 + TEP_th_22, family = "poisson",
               offset = log(nn), data = dd_con))

```

We should do the same for the other couple of variables but since `MFI` is a combination of all covariate except for `TEP_th`, we will drop the synthetic indicator and leave the remainder.

```{r forward selection, warnings = FALSE, echo = FALSE, eval=FALSE}
covariates <- colnames(X)[-c(1, which(colnames(X) %in% c("AES", "MFI")))]
# Covariates included:
covs.in <- c()
# Covariates not included:
BIC.min <- c()
while(length(covs.in) < length(covariates)){
  covs.out <- covariates[which(!covariates %in% covs.in)]

  BICs <- c()
  # At each iteration, we add one of the remaining covariates
  for(j in c(1:length(covs.out))) {
    formula.temp <- paste0("N_ACC ~ 1 + offset(log(nn)) +", 
                           paste(covs.in, collapse = "+"),
                           "+", covs.out[j])
    mod.tmp <- glm(formula.temp, data = dd_con, family = "poisson")
    BICs[j] <- stats::BIC(mod.tmp)
  }
  # Covariate allowing for the best model is added:
  BIC.min <- c(BIC.min, min(BICs))
  covs.in <- c(covs.in, covs.out[which.min(BICs)])
}
```


```{r misc old code, eval = FALSE, echo = FALSE, eval = FALSE}
  #BIC.min <- c(BIC.min, min(BICs))
  #if(length(BIC.min)>1 && BIC.min[length(BIC.min)] >= BIC.min[length(BIC.min)-1]){
  #  break
  #} else{
    #covs.in <- c(covs.in, covs.out[which.min(BICs)])
  #}
```


```{r num covars, echo = FALSE, eval = FALSE}
covs.in[c(1:which.min(BIC.min))]
```

## Nonspatial regression

We regress the counts of accesses $y$ to support centers on the aforementioned explanatory variables. To estimate regression coefficients, all covariates are scaled to zero mean an unit variance. 

\begin{equation}
y_i \mid \eta_i \sim \mathrm{Poisson}(e^{\eta_i + P_i}) \quad \text{where} \quad
\eta_i = X_i^{\top} \beta
\label{eq:glm}
\end{equation}

Where $X$ are the covariate defined earlier, $\beta$ are covariate effects, and $P_i$ is the female population aged $\geq 15$ in municipality $i$.

To gain more insight on the role of all explanatory variables we show the posterior summaries of the full regression model

```{r glm full covs}
cav_glm <- glm(N_ACC ~ 1 +TEP_th_22 + ELI + PGR +
                 UIS + ELL + PDI + ER,
               family = "poisson",offset = log(nn), data = dd_con)
summary(cav_glm)$coefficients
```


  - `TEP_th_22`: The distance from the closest support center seems to play the key role. The easiest interpretation is that the physical distance represents a barrier to violence reporting. This is quite intuitive if we think of the material dynamics of reporting gender-based violence: one could reasonably expect violent men to prevent their partners to come out and report the violence suffered.
  
  - `ELI`: The (ventile of the distribution of the) share of employees in low productivity economic units is a clear indicator of (relative) economic underdevelopment. The most naive interpretation wuld be that in underdeveloped areas reporting gender violence is somewhat harder than in developed ones.

  - `PGR`: The association with population growth rate is harder to interpret. This association is most likely influenced by several demographic instrumental variables we are not keeping into account and would indeed deserve a more dedicated focus.
  
  - `UIS`: The (ventile of the distribution of the) density of production units has a somewhat ambiguous interpretation. From the one side, it has a strong negative relationship with the social frailty index. It should be therefore considered an indicator of economic development. Nevertheless, the regression coefficient bears the same sign as the incidence of low-productivity economic units. *Honestly I have no idea on how to interpret it*.

  - `ELL`: The association with the proportion of people with low educational level has negative sign. The interpretation seems quite easy: cultural development, in general, would encourage reporting violence.
  
  - `PDI`: The association with population dependency index does not seem significantly different from zero
  
  - `ER`: Nor does the association with employment rate.



How do we interpret the size of regression coefficient of `TEP_th_22`? Keeping in mind we are working on the logarithm of the access rate, the standard deviation of the distance, expressed in minutes, is:
```{r SDs of X}
# Distance from closest support center
attr(scale(dists_th_22$TEP_th_22), "scaled:scale")
```


Hence e.g. each $14'6''$ of distance of the a given municipality from the closest support center are associated with a decrease of $0.399$ units in the log-frequency at which women from that municipality access to support centers.



Additionally, the number of zero counts is high:
```{r zero counts}
sum(dd_con$N_ACC == 0)
barplot(table(dd_con$N_ACC))
```

We may wonder if the data generating process incorporates a zero-generating component. We can model this augmented process through zero-inflated Poisson likelihood:
$$
p(y_i | \eta_i) = \pi_0 \mathbb{I}{ \lbrace y_i=0 \rbrace } + (1-\pi_0) 
\frac{e^{-e^{\eta_i + P_i} - (\eta_i + P_i) y_i}}{y_i!}
$$
Where $\pi_0 := \mathrm{Prob} \lbrace y_i = 0 \rbrace$ for all $i$. For the time being, we do not seek for explanatory variables for $\pi_0$.  
When explicitly accounting for the zero-generating process, we find the association of $y$ with some explanatory variables to be reduced:
```{r ZIP fixed}
cav_zip <- pscl::zeroinfl(N_ACC ~ 1 +TEP_th_22 + ELI + PGR +
                            UIS + ELL + PDI + ER | 1, dist = "poisson",
               link = "log", offset = log(nn), data = dd_con)

summary(cav_zip)$coefficients$count
```

However, the MLE estimator for $\pi_0$ is low:
```{r ZIP pizero}
summary(cav_zip)$coefficients$zero
```


## Spatial regression

We plot the log-residuals $\varepsilon$ of the GLM regression model, defined as $\varepsilon := \ln y_i - \ln P_i - \ln \hat{y}_i$ being $\hat{y}_i$ the fitted value.


```{r glm residuals, echo = FALSE, fig.height = 3, fig.cap = "Log-residuals of glm regression using theorical distance as explanatory variable"}


resids_glm_th <- log(dd_con$N_ACC) - log(cav_glm$fitted.values) - log(dd_con$nn)

dd_con %>% 
  dplyr::mutate(log_resids = resids_glm_th) %>% 
  ggplot2::ggplot() +
  ggplot2::geom_sf(ggplot2::aes(fill = .data$log_resids))+
  ggplot2::labs("blank") +
  ggplot2::scale_fill_viridis_c(na.value = "white", direction = -1) +
  ggplot2::theme_classic()

#' Problem: cannot apply Moran test to infinite values!
#' Hence we need to only test autocorrelation across nonzero records.
#' This strongly limits the relevance of the test, if I have a better idea I'll implememnt it.
nonzero <- which(dd_con$N_ACC > 0)

spdep::set.ZeroPolicyOption(TRUE)
suppressWarnings(nb_nonzero <- spdep::poly2nb(dd_con[nonzero, ]))

nonzero_singletons <- which(unlist(lapply(nb_nonzero, function(X) X[1L]==0)))
nonzero_con <- nonzero[-nonzero_singletons]
nb_con_nonzero <- spdep::poly2nb(dd_con[nonzero_con, ])
```

Residuals may exhibit spatial structure. To assess it, we employ the Moran and Geary tests. Since 

Please notice that log-residuals only take finite values across the $175$ municipalities whose female citizens have reported at least one case of violence in 2022.

Additionally, this set of municipalities includes $2$ singletons, which we remove to assess the value of the Moran and Geary statistics. Thus, we have defined the indexes set `nonzero_con` as the set of municipalities from which at least one case of gender-based violence has been reported, *and* which have at least one neighbouring municipalities from which at least one case of gender-based violence was reported. 

```{r moran residuals}
spdep::moran.test(resids_glm_th[nonzero_con],
                  listw = spdep::nb2listw(nb_con_nonzero))
spdep::geary.test(resids_glm_th[nonzero_con],
                  listw = spdep::nb2listw(nb_con_nonzero))

```

In both cases, we find evidence for spatial autocorrelation. However, we must stress out this result does not refer to all the regional territory, but only to a subset of all municipalities ($173$ over $257$)


Based on the autocorrelation evidence, though it has only been assessed for a subset of all municipalities, we try implementing some simple spatial models by adding a conditionally autoregressive latent effect, say $z$, to the linear predictor


\begin{equation}
\eta_i = X_i^{\top} \beta + z_i 
\label{eq:mspat}
\end{equation}


We test a total of four models, all of which have a prior distribution depending on the spatial structure of the underlying graph, in this case the Apulia region. 

We describe the spatial structure starting from municipality neighbourhood, and introduce the neighbourhood matrix $W$, whose generic element $w_{ij}$ takes value $1$ if municipalities $i$ and $j$ are neighbours and $0$ otherwise. For each $i \in [1,n]$, $d_i:= \sum_{j=1}^n w_{ij}$ is the number of neigbours of $i$-th municipality. Plase notice we have have $n = 256$.

For all models, we define $\sigma^2$ as the scale parameter of the latent effect, and in order to avoid overfitting we set a PC-prior on it with rate parameter $\lambda = 1.5$, such that $\mathrm{Prob}(\sigma>\lambda) = 0.01$

Spatial models are computed by approximating the marginal posteriors of interest via the Integrated Nested Laplace Approximation (INLA), adopting the novel Variational Bayes Approach \cite{INLAVB}.



#### ICAR model

The Intrinsic CAR model is the simplest formulation among spatial autoregressive models. The conditional distribution of each value $z_i \mid z_{-i}$ is:

\begin{equation}
z_i \mid z_{-i} \sim N \left( \sum_{j=1}^n \frac{w_{ij}}{d_i} z_j \,, \frac{\sigma^2}{d_i}\right)
\label{eq:icar_loc}
\end{equation}

Since the joint distribution of $z$ is improper, a sum-to-zero constraint is required for identifiability.

```{r inla nosp, echo = FALSE, message = FALSE, warning = FALSE}

library(INLA)
cav_nosp_INLA <- inla(
  N_ACC ~ 1 +TEP_th_22 + ELI + PGR + UIS + ELL + PDI + ER,
  family = "poisson", offset = log(nn), data =dd_con,
  num.threads = 1, control.compute = list(internal.opt = F, cpo = T, waic = T), 
  inla.mode = "classic", control.inla = list(strategy = "laplace"),
  verbose = F)

cav_nosp_INLA_ZIP <- inla(
  N_ACC ~ 1 +TEP_th_22 + ELI + PGR + UIS + ELL + PDI + ER,
  family = "zeroinflatedpoisson1", offset = log(nn), data =dd_con,
  num.threads = 1, control.compute = list(internal.opt = F, cpo = T, waic = T), 
  inla.mode = "classic", control.inla = list(strategy = "laplace"),
  verbose = F)

```



```{r inla mod, echo = FALSE, message = FALSE, warning = FALSE}

cav_icar_INLA <- inla(
  N_ACC ~ 1 +TEP_th_22 + ELI + PGR + UIS + ELL + PDI + ER+ f(ID, model = "besag", graph = W_con,
                         scale.model = T, prior = "pc.prec", param = c(1.5, 0.01)),
  family = "poisson", offset = log(nn), data =dd_con,
  num.threads = 1, control.compute = list(internal.opt = F, cpo = T, waic = T), 
  inla.mode = "classic", control.inla = list(strategy = "laplace"),
  verbose = F) 

cav_icar_INLA_ZIP <- inla(
  N_ACC ~ 1 +TEP_th_22 + ELI + PGR + UIS + ELL + PDI + ER+ f(ID, model = "besag", graph = W_con,
                         scale.model = T, prior = "pc.prec", param = c(1.5, 0.01)),
  family = "zeroinflatedpoisson1", offset = log(nn), data =dd_con,
  num.threads = 1, control.compute = list(internal.opt = F, cpo = T, waic = T), 
  inla.mode = "classic", control.inla = list(strategy = "laplace"),
  verbose = F) 
```



#### PCAR model

The intrinsic autoregressive model is relatively simple to interpret and to implement, 
while also requiring the minimum number of additional parameter (either the scale or the precision).


The drawback, however, is that we implicitly assume a deterministic spatial autocorrelation coefficient equal to 1.
When the autocorrelation is weak, setting an ICAR prior may be a form of misspecification.

A generalisation of this model is the PCAR (proper CAR), which introduces an autocorrelation parameter $\alpha$:


\begin{equation}
z_i \mid z_{-i} \sim N \left( \sum_{j=1}^n \alpha \frac{w_{ij}}{d_i} z_j \,, \frac{\sigma^2}{d_i}\right)
\label{eq:pcar_loc}
\end{equation}

The `R` code to implement the PCAR model in `R-INLA` is in Appendix.
```{r pcar definition, echo = FALSE}
inla.rgeneric.PCAR.model <- 
  function (cmd = c("graph", "Q", "mu", "initial", "log.norm.const", 
                                              "log.prior", "quit"), theta = NULL) {
  interpret.theta <- function() {
    alpha <- 1/(1 + exp(-theta[1L])) # alpha modelled in logit scale
    mprec <- sapply(theta[2L], function(x) {
      exp(x)
    })
    PREC <- mprec
    return(list(alpha = alpha, mprec = mprec, PREC = PREC))
  }
  graph <- function() {
    G <- Matrix::Diagonal(nrow(W), 1) + W
    return(G)
  }
  Q <- function() {
    param <- interpret.theta()
    Q <- param$PREC * 
      (Matrix::Diagonal(nrow(W), apply(W, 1, sum)) - param$alpha * W)
    return(Q)
  }
  mu <- function() {
    return(numeric(0))
  }
  log.norm.const <- function() {
    val <- numeric(0)
    return(val)
  }
  log.prior <- function() {
    param <- interpret.theta()
    val <- -theta[1L] - 2 * log(1 + exp(-theta[1L]))
    # # PC prior
    val <- val + log(lambda/2) - theta[2L]/2 - (lambda * exp(-theta[2L]/2))
    # # Gamma(1, 5e-5), default prior:
    #val <- val + dgamma(exp(theta[2L]), shape = 1, rate = 5e-5, log = T) + theta[2L]
    # # Uniform prior on the standard deviation
    #val <- val - sum(theta[2L])/2 - k * log(2)
    return(val)
  }
  initial <- function() {
    return(c(0, 4))
  }
  quit <- function() {
    return(invisible())
  }
  if (as.integer(R.version$major) > 3) {
    if (!length(theta)) 
      theta = initial()
  }
  else {
    if (is.null(theta)) {
      theta <- initial()
    }
  }
  val <- do.call(match.arg(cmd), args = list())
  return(val)
}
PCAR.model <- function(...) INLA::inla.rgeneric.define(inla.rgeneric.PCAR.model, ...)
```

```{r pcar run, message = FALSE, echo = FALSE, output = FALSE}
cav_pcar_INLA<- inla(N_ACC ~ 1 +TEP_th_22 + ELI + PGR + UIS + ELL + PDI + ER + 
                           f(ID, model = PCAR.model(W = W_con, k = 1, lambda = 1.5)),
                         family = "poisson", offset = log(nn), data =dd_con,
                         num.threads = 1, control.compute = 
                           list(internal.opt = F, cpo = T, waic = T), 
                         #inla.mode = "classic", control.inla = list(strategy = "laplace"),
                         control.predictor = list(compute = T),
                         verbose = T) 

cav_pcar_INLA_ZIP <- inla(N_ACC ~ 1 +TEP_th_22 + ELI + PGR + UIS + ELL + PDI + ER + 
                           f(ID, model = PCAR.model(W = W_con, k = 1, lambda = 1.5)),
                         family = "zeroinflatedpoisson1", offset = log(nn), data =dd_con,
                         num.threads = 1, control.compute = 
                           list(internal.opt = F, cpo = T, waic = T), 
                         #inla.mode = "classic", control.inla = list(strategy = "laplace"),
                         control.predictor = list(compute = T),
                         verbose = T) 
```

We show the posterior summary for the autocorrelation coefficient. 

```{r alfa, echo = FALSE}
expit <- function (X){
  return(1/(1 + exp(-X)))
}

inla.zmarginal(inla.tmarginal(fun = expit,
                              marginal = cav_pcar_INLA$marginals.hyperpar[[1]]))
```



#### BYM model


Perhaps, our data are generated by a process dominated from noise. We can thus try a different path: the BYM model. On a preliminary stance, we keep trusting in the accuracy of the Laplace approximation and stick to INLA. On a later stage, it would be more rigorous to compare INLA results to the posteriors of a model estimated with MCMC.

The BYM model we employ follows the parametrisation of [@BYM2]:

\begin{equation}
z_i = \sigma \left(\sqrt{\phi} u_i + \sqrt{1-\phi} v_i \right)
\label{eq_bym2}
\end{equation}

where $u$ is an ICAR field, $v$ is an IID standard Gaussian white noise i.e. $v \sim N(0, I)$, and $\phi$ is a mixing parameter $\in [0, 1]$.

```{r INLA bym, echo = FALSE}

cav_bym_INLA <- inla(
  N_ACC ~ 1 +TEP_th_22 + ELI + PGR + UIS + ELL + PDI + ER+ f(ID, model = "bym2", graph = W_con,
                         scale.model = T, prior = "pc.prec", param = c(1.5, 0.01)),
  family = "poisson", offset = log(nn), data =dd_con,
  num.threads = 1, control.compute = list(internal.opt = F, cpo = T, waic = T), 
  #inla.mode = "classic", control.inla = list(strategy = "laplace"),
  verbose = F)

cav_bym_INLA_ZIP <- inla(
  N_ACC ~ 1 +TEP_th_22 + ELI + PGR + UIS + ELL + PDI + ER+ f(ID, model = "bym2", graph = W_con,
                         scale.model = T, prior = "pc.prec", param = c(1.5, 0.01)),
  family = "zeroinflatedpoisson1", offset = log(nn), data =dd_con,
  num.threads = 1, control.compute = list(internal.opt = F, cpo = T, waic = T), 
  #inla.mode = "classic", control.inla = list(strategy = "laplace"),
  verbose = F)
```

#### Leroux model

As an alternative to take into account both structured and unstructured latent effects, we also test the Leroux autoregressive model [@Leroux]. In this case, the local prior for $z_i$ is

\begin{equation}
z_i \mid z_{-i} \sim N \left( \sum_{j=1}^n \frac{\xi w_{ij}}{1 - \xi + \xi d_i}   z_j \,, \frac{\sigma^2}{1 - \xi + \xi d_i}\right)
\label{eq:leroux_loc}
\end{equation}

Where $\xi \in [0, 1]$ is the mixing parameter. A more interesting representation of the Leroux model is the joint prior
$$
z \mid \sigma^2, \xi \sim N(0, \sigma^2[\xi R + (1-\xi)I]^{-1})
$$
where $R := D-W$ is the graph Laplacian matrix, $W$ is the neighbourhood matrix and $D$ is the corresponding degree matrix.


```{r INLA Leroux, echo = FALSE}
cav_leroux_INLA <-   inla(N_ACC ~ 1 +TEP_th_22 + ELI + PGR + UIS + ELL + PDI + ER +
                          f(ID, model = "besagproper2", graph = W_con, 
                            hyper = list(prec = list(prior = "pc.prec", param = c(1.5, 0.01)))),
                     family = "poisson", offset = log(nn), data =dd_con,
                     num.threads = 1, control.compute = 
                       list(internal.opt = F, cpo = T, waic = T), 
                     #inla.mode = "classic", control.inla = list(strategy = "laplace"),
                     control.predictor = list(compute = T),
                     verbose = F) 

cav_leroux_INLA_ZIP <- inla(N_ACC ~ 1 +TEP_th_22 + ELI + PGR + UIS + ELL + PDI + ER +
                          f(ID, model = "besagproper2", graph = W_con, 
                            hyper = list(prec = list(prior = "pc.prec", param = c(1.5, 0.01)))),
                     family = "zeroinflatedpoisson1", offset = log(nn), data =dd_con,
                     num.threads = 1, control.compute = 
                       list(internal.opt = F, cpo = T, waic = T), 
                     #inla.mode = "classic", control.inla = list(strategy = "laplace"),
                     control.predictor = list(compute = T),
                     verbose = F) 

```



#### Comparison

We briefly compare these four through the WAIC [@GelmanWAIC]:

```{r WAICs table, echo = F}

WAICS <- tibble::tibble(
  Model = c("Null", "ICAR", "PCAR", "BYM", "Leroux"),
  WAIC_Poisson = round(c(
    cav_nosp_INLA$waic$waic,  cav_icar_INLA$waic$waic,  cav_pcar_INLA$waic$waic, 
    cav_bym_INLA$waic$waic,  cav_leroux_INLA$waic$waic),3),
  WAIC_ZIP = round(c(
    cav_nosp_INLA_ZIP$waic$waic, cav_icar_INLA_ZIP$waic$waic, cav_pcar_INLA_ZIP$waic$waic,
    cav_bym_INLA_ZIP$waic$waic, cav_leroux_INLA_ZIP$waic$waic),3),  
  Eff_params_Poisson = round(c(
    cav_nosp_INLA$waic$p.eff,   cav_icar_INLA$waic$p.eff,  cav_pcar_INLA$waic$p.eff,
    cav_bym_INLA$waic$p.eff, cav_leroux_INLA$waic$p.eff),3),
  Eff_params_ZIP = round(c(
    cav_nosp_INLA_ZIP$waic$p.eff,  cav_icar_INLA_ZIP$waic$p.eff,  cav_pcar_INLA_ZIP$waic$p.eff,
    cav_bym_INLA_ZIP$waic$p.eff,  cav_leroux_INLA_ZIP$waic$p.eff),3) )

WAICS

```
As we can see, models taking into account random noise have a better performance. 

## Model results

Here, posterior estimates of $\beta$ under the BYM model are shown. We compare the results assuming both the Poisson and the ZIP likelihood:

```{r bym summary fixed, echo = F}
betas <- cbind(
  tibble::tibble(Variable = rownames(cav_bym_INLA$summary.fixed)),
  round(tibble::tibble(
  Mean_Pois = cav_bym_INLA$summary.fixed$mean,
  Mean_ZIP = cav_bym_INLA_ZIP$summary.fixed$mean,
  SD_Pois = cav_bym_INLA$summary.fixed$sd,
  SD_ZIP = cav_bym_INLA_ZIP$summary.fixed$sd,
  q0.025_Pois = cav_bym_INLA$summary.fixed$`0.025quant`,
  q0.025_ZIP = cav_bym_INLA_ZIP$summary.fixed$`0.025quant`,
  q0.975_Pois = cav_bym_INLA$summary.fixed$`0.975quant`,
  q0.975_ZIP = cav_bym_INLA_ZIP$summary.fixed$`0.975quant`), 3))



betas
```

```{r plot beta, echo = FALSE}

betalimX <-range(do.call(c, lapply(cav_bym_INLA$marginals.fixed[-1], FUN = function(x) x[,1])))
betalimY <-range(do.call(c, lapply(cav_bym_INLA$marginals.fixed[-1], FUN = function(x) x[,2])))

betamarg <- as.data.frame(do.call(cbind, cav_bym_INLA$marginals.fixed[-1]))
names(betamarg) <- paste0(rep(names(cav_bym_INLA$marginals.fixed[-1]),each=2), c("__X", "__Y"))


betamarg_long <- tidyr::pivot_longer( betamarg, cols = c(1:14),
  names_to = c("Effect", ".value"),# Split names into "parameter" and value column names
  names_sep = "__")

ggplot2::ggplot(betamarg_long, ggplot2::aes(x = .data$X, y = .data$Y, color = .data$Effect)) +
  ggplot2::geom_line(size = 0.7) +
  ggplot2::geom_vline(xintercept = 0)+
  ggplot2::coord_cartesian(xlim = betalimX, ylim = betalimY) +
  ggplot2::labs(
    title = "Posterior Marginals of covariates effects",
    x = expression(beta),
    y = expression(pi(beta ~ "|" ~ y)),
    color = "Effect") +
  ggplot2::theme_classic()


```


Estimations of $\beta$ differ from the nonspatial model. For all variables, credibility intervals are wider due to increased uncertainty. 

  - `TEP_th_22` The effect of the distance from the closest support center remains similar in mean and the interpretation is not altered.

  - `ELI`: The effect of the incidence of low-productivity economic units is shrunk in mean while its variability increases

  - `PGR`: The association with population growth rate is still positive and significantly $\neq 0$
  
  - `UIS`: The association with the density of productive units appears not significant, due to increased variability
  
  - `ELL`: The association with the incidence of low education levels, instead, is doubled in mean. We interpret this result as a strong *potential* impact of education on the chance that gender violence is reported
  
  - `PDI`: The effect of structural dependency index is utterly negligible
  
  - `ER`: The effect associated with employment rate is more than doubled in mean with respect to the nonspatial model. How to interpret this finding? Employment rate is clearly an indicator of economic development, hence the easiest interpretation is that - as it was with `ELI` under the nonspatial model - in more developed areas there is a higher chance that gender violence is reported.
  
Lastly, we take a look at model hyperparameters:
```{r bym hyperpar, echo = F}

cav_bym_INLA_marginals.var <- unlist(inla.zmarginal(inla.tmarginal(
  fun = function(x) 1/x,marginal =cav_bym_INLA$marginals.hyperpar[[1]]), silent = T))

cav_bym_INLA_ZIP_marginals.var <- unlist(inla.zmarginal(inla.tmarginal(
  fun = function(x) 1/x,marginal =cav_bym_INLA$marginals.hyperpar[[2]]), silent = T))

hyperpars<- data.frame(rbind(cav_bym_INLA_marginals.var[c(1,3,5,7)],
                   cav_bym_INLA$summary.hyperpar[2,c(1,3,4,5)],
                  cav_bym_INLA_ZIP_marginals.var[c(1,3,5,7)],
                   cav_bym_INLA_ZIP$summary.hyperpar[c(1,3),c(1,3,4,5)])) %>% 
  dplyr::mutate(Variable =c("Z_VAR_Pois", "Mixing_Pois","Z_VAR_ZIP", "Zero_Prob",  "Mixing_ZIP")) %>% 
                  dplyr::relocate(.data$Variable, .before = 1) 

rownames(hyperpars) <- NULL

hyperpars

```
Even though the value of the zero-inflation parameter is low, we can see deep differences in the structure of the latent effects: the precision parameter median of the ZIP model is almost double than under the Poisson model, while the mixing parameter is shrunk.

For completeness, we show the hyperparameters posterior summary also for the Leroux model. The mixing parameter is not directly comparable. Neither does the precision parameter, since only under intrinsic models can the Laplacian matrix be scaled *a priori*. That being said, we see the difference between the two likelihoods is analogous.

```{r leroux hyperpar, echo = FALSE, warning = FALSE, message = FALSE}

cav_leroux_INLA_marginals.var <- unlist(inla.zmarginal(inla.tmarginal(
  fun = function(x) 1/x,marginal =cav_leroux_INLA$marginals.hyperpar[[1]]), silent = T))

cav_leroux_INLA_ZIP_marginals.var <- unlist(inla.zmarginal(inla.tmarginal(
  fun = function(x) 1/x,marginal =cav_leroux_INLA$marginals.hyperpar[[2]]), silent = T))

hyperpars_leroux <- data.frame(rbind(cav_leroux_INLA_marginals.var[c(1,3,5,7)],
                   cav_leroux_INLA$summary.hyperpar[2,c(1,3,4,5)],
                  cav_leroux_INLA_ZIP_marginals.var[c(1,3,5,7)],
                   cav_leroux_INLA_ZIP$summary.hyperpar[c(1,3),c(1,3,4,5)])) %>% 
  dplyr::mutate(Variable =c("Z_VAR_Pois", "Mixing_Pois", "Z_VAR_ZIP", "Zero_Prob",  "Mixing_ZIP")) %>% 
                  dplyr::relocate(.data$Variable, .before = 1) 

rownames(hyperpars_leroux) <- NULL

hyperpars_leroux
```

It is, however, worth noticing how INLA manages to meet the regularity conditions to approximate the CPO more often than in the non-inflated model. Still, CPO must be re-computed manually before employing it as an evaluation metrics:
```{r cpo failures}

sum(cav_bym_INLA$cpo$failure)
sum(cav_bym_INLA$cpo$failure > 0)


sum(cav_bym_INLA_ZIP$cpo$failure)
sum(cav_bym_INLA_ZIP$cpo$failure > 0)

sum(cav_leroux_INLA$cpo$failure)
sum(cav_leroux_INLA$cpo$failure > 0)


sum(cav_leroux_INLA_ZIP$cpo$failure)
sum(cav_leroux_INLA_ZIP$cpo$failure > 0)
```

Lastly, we plot the estimated latent BYM effect under the Poisson model
```{r zhat bym, echo = FALSE}
dd_con %>% 
  dplyr::mutate(zhat = cav_bym_INLA$summary.random$ID$mean[c(1:nrow(dd_con))]) %>% 
  ggplot2::ggplot() +
  ggplot2::geom_sf(ggplot2::aes(fill = .data$zhat))+
  ggplot2::labs("Expected BYM spatial effect") +
  ggplot2::scale_fill_viridis_c(na.value = "white", direction = -1) +
  ggplot2::theme_classic()
```





## Weakness elements and possible developments

From this preliminary analysis, inference on spatial models is hindered by the dominance of random noise over structured spatial effects. This can be argued from the posterior distribution of the mixing parameter in the BYM model, other than from the low spatial autocorrelation parameter in the PCAR.

This means that only to a small extent the variation in $y$ not explained by covariates can be explained by spatial structure.

On the other hand, it is difficult to assert *all* variation not explained by covariates is pure noise, otherwise we would have evidence for the lack of autocorrelation in residuals. We tested the hypothesis of no autocorrelation in GLM residuals by the Moran's $I$ test, but in doing so we had to only test the residuals of areas with nonzero counts.

Moreover, spatial models are estimated using the INLA. While this is a broadly employed approach in epidemiology and in disease mapping, so far we did not assess how accurate the Laplace approximation has been.

To do so, we should e.g. rerun the same models using MCMC methods, e.g. using  `R` libraries such as `CARBayes`, and replicating the same prior structure used.

Lastly, we did *not* model the rate at which gender violence occurs, but the occurrence of violence reports. Higher occurrence of violence reports from a given territory may thus depend on two factors: either the higher occurrence of violence in that territory, or the ease in reporting violence for the residents. 

Whereas the easiest interpretation is that violence occurrence is underestimated in low-reporting areas, at the time being nothing prevents us from suspecting that the placement of support centers is at least partially strategic, i.e. the distribution of supporting centers is more dense in areas in which violence occurs, for some reason we don't know, aa a higher frequence. 

## Appendix: the WAIC

Following \cite{GelmanWAIC}, the WAIC is given by the sum of two components:

$$
WAIC := 2 \sum_{i=1}^n \mathrm{VAR}[\ln p(y_i |\theta)]
- 2 \sum_{j=1}^n \ln \mathbb{E}[p(y_i | \theta)]
$$
Where $\theta$ is the full set of model parameters; the variance and the average are computed by integrating over the posterior of $\theta$.
The first addendum denotes the number of free parameters, while the second term is a measure for goodness of fit. 

## Appendix: R code to implement the PCAR model in `INLA`

Although it is not readily implemented in `R-INLA` (the `"besagproper"` effect is actually the Leroux model) we may base the R code on the `INLAMSM`` package [@INLAMSM]:
```{r pcar display code, eval = FALSE}
inla.rgeneric.PCAR.model <- 
  function (cmd = c("graph", "Q", "mu", "initial", "log.norm.const", 
                                              "log.prior", "quit"), theta = NULL) {
  interpret.theta <- function() {
    alpha <- 1/(1 + exp(-theta[1L])) # alpha modelled in logit scale
    mprec <- sapply(theta[2L], function(x) {
      exp(x)
    })
    PREC <- mprec
    return(list(alpha = alpha, mprec = mprec, PREC = PREC))
  }
  graph <- function() {
    G <- Matrix::Diagonal(nrow(W), 1) + W
    return(G)
  }
  Q <- function() {
    param <- interpret.theta()
    Q <- param$PREC * 
      (Matrix::Diagonal(nrow(W), apply(W, 1, sum)) - param$alpha * W)
    return(Q)
  }
  mu <- function() {
    return(numeric(0))
  }
  log.norm.const <- function() {
    val <- numeric(0)
    return(val)
  }
  log.prior <- function() {
    param <- interpret.theta()
    val <- -theta[1L] - 2 * log(1 + exp(-theta[1L]))
    # # PC prior
    val <- val + log(lambda/2) - theta[2L]/2 - (lambda * exp(-theta[2L]/2))
    # # Gamma(1, 5e-5), default prior:
    #val <- val + dgamma(exp(theta[2L]), shape = 1, rate = 5e-5, log = T) + theta[2L]
    # # Uniform prior on the standard deviation
    #val <- val - sum(theta[2L])/2 - k * log(2)
    return(val)
  }
  initial <- function() {
    return(c(0, 4))
  }
  quit <- function() {
    return(invisible())
  }
  if (as.integer(R.version$major) > 3) {
    if (!length(theta)) 
      theta = initial()
  }
  else {
    if (is.null(theta)) {
      theta <- initial()
    }
  }
  val <- do.call(match.arg(cmd), args = list())
  return(val)
  }

PCAR.model <- function(...) INLA::inla.rgeneric.define(inla.rgeneric.PCAR.model, ...)
```


## Bibliography






---
title: "Exploratory analysis of accesses to support centers for gender-based violence
  in Apulia"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning= FALSE)
par(mar = c(2,2,1,1))
library(magrittr)
library(sf)
library(INLA)
library(INLAMSM)
#if(!rlang::is_installed("pscl")) install.packages("pscl")
```

```{r input, echo = FALSE, message = FALSE, warning = FALSE, output = FALSE}
## Input ------------------------------------------------------------------f-----

#' NEVER forget calling magrittr and sf. Never --------------------------------#

library(magrittr)
library(sf)

load("input/CAV_input_mun_2021.RData")
load("input/CAV_input_mun_2022.RData")
load("input/CAV_input_mun_2023.RData")

CAV_mun_21 <- CAV_mun_21 %>% dplyr::rename(N_ACC_21 = .data$N_ACC)
CAV_mun_22 <- CAV_mun_22 %>% dplyr::rename(N_ACC_22 = .data$N_ACC)
CAV_mun_23 <- CAV_mun_23 %>% dplyr::rename(N_ACC_23 = .data$N_ACC)


load("input/Shp.RData")

#'  Function to extract numeric digits from a strings vector (needed to filter age):
nn_extract <- function(string){
  nn <- gregexpr("([0-9])", string)
  ls.out <- regmatches(as.list(string), nn)
  res <- unlist(lapply(ls.out, function(x) as.numeric(paste(x, collapse = ""))))  
  return(res)
}

# Female population aged >= 15 years.
# Source data: http://dati.istat.it/Index.aspx?DataSetCode=DCIS_POPRES1#
Popolazione_Puglia_2021 <- readr::read_csv("input/Popolazione_Puglia_2021.csv") %>% 
  dplyr::select(.data$ITTER107, .data$Territorio, .data$SEXISTAT1, .data$ETA1, .data$Value) %>% 
  dplyr::filter(.data$ETA1 != "TOTAL") %>% 
  dplyr::mutate(ETA1 = nn_extract(ETA1)) %>% 
  dplyr::mutate(ITTER107 = as.numeric(ITTER107))

Popolazione_Puglia_2022 <- readr::read_csv("input/Popolazione_Puglia_2022.csv") %>% 
  dplyr::select(.data$ITTER107, .data$Territorio, .data$SEXISTAT1, .data$ETA1, .data$Value) %>% 
  dplyr::filter(.data$ETA1 != "TOTAL") %>% 
  dplyr::mutate(ETA1 = nn_extract(ETA1)) %>% 
  dplyr::mutate(ITTER107 = as.numeric(ITTER107))

Popolazione_Puglia_2023 <- readr::read_csv("input/Popolazione_Puglia_2023.csv") %>% 
  dplyr::select(.data$ITTER107, .data$Territorio, .data$SEXISTAT1, .data$ETA1, .data$Value) %>% 
  dplyr::filter(.data$ETA1 != "TOTAL") %>% 
  dplyr::mutate(ETA1 = nn_extract(ETA1)) %>% 
  dplyr::mutate(ITTER107 = as.numeric(ITTER107))




# Filter and aggregate:
Pop_f_15 <- Popolazione_Puglia_2021 %>% 
  dplyr::left_join(Popolazione_Puglia_2022,
                   by = c("ITTER107", "Territorio", "SEXISTAT1", "ETA1")) %>% 
  dplyr::left_join(Popolazione_Puglia_2023,
                   by = c("ITTER107", "Territorio", "SEXISTAT1", "ETA1")) 

names(Pop_f_15) <-  c("PRO_COM", "Comune", "Sesso", "Eta", 
                      "Popolazione_21", "Popolazione_22", "Popolazione_23")


Pop_f_15 <- Pop_f_15 %>% dplyr::filter(.data$Sesso == 2) %>% 
  dplyr::filter(.data$Eta > 14) %>% 
  dplyr::group_by(.data$PRO_COM, .data$Comune) %>% 
  dplyr::summarise(nn21 = sum(.data$Popolazione_21),
                   nn22 = sum(.data$Popolazione_22),
                   nn23 = sum(.data$Popolazione_23)) %>%
  dplyr::ungroup()




# Complete dataset:
dd <- Shp %>% dplyr::left_join(Pop_f_15[, -2],
                               by = "PRO_COM") %>% 
  dplyr::left_join(dplyr::select(CAV_mun_21, -.data$comune),by = "PRO_COM") %>% 
  dplyr::left_join(dplyr::select(CAV_mun_22, -.data$comune),by = "PRO_COM") %>% 
  dplyr::left_join(dplyr::select(CAV_mun_23, -.data$comune),by = "PRO_COM") %>% 
  dplyr::left_join(dplyr::select(Indicators, -.data$Comune), by = "PRO_COM")

# Municipalities from which no woman reported violence --> count == 0
dd$N_ACC_21[is.na(dd$N_ACC_21)] <- 0
dd$N_ACC_22[is.na(dd$N_ACC_22)] <- 0 
dd$N_ACC_23[is.na(dd$N_ACC_23)] <- 0 

# "access ratio"
dd$LN_ACC_21 <- log(dd$N_ACC_21/dd$nn21)
dd$LN_ACC_22 <- log(dd$N_ACC_22/dd$nn22)
dd$LN_ACC_23 <- log(dd$N_ACC_23/dd$nn23)


ggy21 <- ggplot2::ggplot() +
  ggplot2::geom_sf(data = dd, 
                   ggplot2::aes(fill = .data$LN_ACC_21))+
  ggplot2::scale_fill_viridis_c(na.value = "white",
                                direction = -1,
                                limits = c(-9.5, -4.5))+
  ggplot2::theme_classic()

ggy22 <- ggplot2::ggplot() +
  ggplot2::geom_sf(data = dd, 
                   ggplot2::aes(fill = .data$LN_ACC_22))+
  ggplot2::scale_fill_viridis_c(na.value = "white",
                                direction = -1,
                                limits = c(-9.5, -4.5))+
  ggplot2::theme_classic()

ggy23 <- ggplot2::ggplot() +
  ggplot2::geom_sf(data = dd, 
                   ggplot2::aes(fill = .data$LN_ACC_23))+
  ggplot2::scale_fill_viridis_c(na.value = "white",
                                direction = -1,
                                limits = c(-9.5, -4.5))+
  ggplot2::theme_classic()



load("input/dists_th_22.RData")
load("input/dists_th_23.RData")

# Tremiti Islands are a singleton --> need to remove them to perform spatial analysis
suppressWarnings({
  singletons <- which(unlist(lapply(spdep::poly2nb(dd), function(x) x[1L] == 0)))
})

# This is the dataset we will concretely work on.
# Covariates are all scaled to zero mean and unit variance
dd_con <- dd[-singletons, ] %>% 
  dplyr::left_join(dists_th_22, by = "PRO_COM") %>% 
  dplyr::left_join(dists_th_23, by = "PRO_COM") %>% 
  dplyr::mutate(TEP_th_22 = as.vector(scale(.data$TEP_th_22))) %>% 
  dplyr::mutate(TEP_th_23 = as.vector(scale(.data$TEP_th_23))) %>% 
  dplyr::mutate(AES = as.vector(scale(.data$AES))) %>% 
  dplyr::mutate(MFI = as.vector(scale(.data$MFI)))  %>% 
  dplyr::mutate(PDI = as.vector(scale(.data$PDI)))  %>% 
  dplyr::mutate(ELL = as.vector(scale(.data$ELL)))  %>% 
  dplyr::mutate(ER = as.vector(scale(.data$ER)))  %>% 
  dplyr::mutate(PGR = as.vector(scale(.data$PGR)))  %>% 
  dplyr::mutate(UIS = as.vector(scale(.data$UIS)))  %>% 
  dplyr::mutate(ELI = as.vector(scale(.data$ELI))) 


# neighbours list
nb_con <- spdep::poly2nb(dd_con)
# neighbouring/adjacency matrix
W_con <- spdep::nb2mat(nb_con, style = "B")
rownames(W_con) <- colnames(W_con) <- dd_con$PRO_COM

# Laplacian matrix:
Lapl_con <- diag(rowSums(W_con)) - W_con
V_con <- eigen(Lapl_con)$vectors


# row ID - needed for spatial models
dd_con$ID <- c(1:nrow(dd_con))


glm_all_X <- glm(N_ACC_21 ~ 1 + TEP_th_22 + MFI + AES + PDI + ELL + ER +
                   PGR + UIS + ELI + offset(log(nn21)),
                 data = dd_con, family = "poisson")
# model matrix
X <- model.matrix(glm_all_X)

n <- nrow(dd_con)
dd_list <- list (
  Intercept = matrix(c(rep(1, n), rep(NA, 3*n), 
                 rep(1,n), rep(NA, 3*n), 
                 rep(1,n)), nrow = 3*n, 
                 ncol = 3, byrow = FALSE),
  N_ACC = matrix(c(dd_con$N_ACC_21, rep(NA, 3*n), 
                 dd_con$N_ACC_22, rep(NA, 3*n), 
                 dd_con$N_ACC_23), nrow = 3*n, 
                 ncol = 3, byrow = FALSE),
  TEP_th = matrix(c(dd_con$TEP_th_22, rep(NA, 3*n), 
                      dd_con$TEP_th_22, rep(NA, 3*n), 
                      dd_con$TEP_th_23), nrow = 3*n,
                  ncol = 3, byrow = FALSE),
  ELI = matrix(c(dd_con$ELI, rep(NA, 3*n), 
                dd_con$ELI, rep(NA, 3*n), 
                dd_con$ELI), nrow = 3*n,
              ncol = 3, byrow = FALSE),
  PGR = matrix(c(dd_con$PGR, rep(NA, 3*n), 
                dd_con$PGR, rep(NA, 3*n), 
                dd_con$PGR), nrow = 3*n,
              ncol = 3, byrow = FALSE),
  UIS = matrix(c(dd_con$UIS, rep(NA, 3*n), 
                dd_con$UIS, rep(NA, 3*n), 
                dd_con$UIS), nrow = 3*n,
              ncol = 3, byrow = FALSE),
  ELL = matrix(c(dd_con$ELL, rep(NA, 3*n), 
                 dd_con$ELL, rep(NA, 3*n), 
                 dd_con$ELL), nrow = 3*n,
               ncol = 3, byrow = FALSE),
  PDI = matrix(c(dd_con$PDI, rep(NA, 3*n), 
                dd_con$PDI, rep(NA, 3*n), 
                dd_con$PDI), nrow = 3*n,
              ncol = 3, byrow = FALSE),
  ER = matrix(c(dd_con$ER, rep(NA, 3*n), 
                dd_con$ER, rep(NA, 3*n), 
                dd_con$ER), nrow = 3*n,
              ncol = 3, byrow = FALSE),
  ID = c(1:n, (n + c(1:n)), (2*n + c(1:n))),
  nn = c(dd_con$nn21, dd_con$nn22, dd_con$nn23)) 

dd_list_st <- dd_list

names(dd_list_st)[which(names(dd_list_st)=="ID")] <- "Area"

dd_list_st$Area <- (dd_list_st$Area - 1)%%nrow(dd_con) + 1
dd_list_st$Year <- rep(c(1,2,3), each = nrow(dd_con))

dd_list_st$Area_bis <- dd_list_st$Area
dd_list_st$Year_bis <- dd_list_st$Year
dd_list_st$ID <- dd_list_st$Area + (dd_list_st$Year-1)*nrow(dd_con)

```


## Data
The dataset employed regards the counts of accesses to gender-based violence support centers in the Apulia region by residence municipality of the women victims of violence during 2022. `R` codes to generate the dataset are in the R script [posted here](https://github.com/lcef97/CAV_Puglia/blob/main/CAV_full.R) which this report is based on. Observational period are years 2021, 2022, 2023.

Here, we only take into account the violence reports which support centers actually take charge of, at the risk of underestimating the counts of gender-based violence cases.
This choice is driven by the need of avoiding duplicated records, since e.g. it may happen that a support center redirects a victim to another support center. 

In order to avoid singletons in the spatial structure of the dataset, we removed the Tremiti Islands from the list of municipalities included ($0$ accesses recorded so far).

Therefore, the municipality-level dataset in scope consists of $256$ observations. 

We can only take into account the accesses to support centers for which the origin municipality of victims is reported; therefore the total count of accesses in scope is $1477$, $1516$ and $1822$ for the three reference years respectively. 

Here, we plot the log-access rate per residence municipality, i.e. the logarithm of the ratio between access counts and female population. Blank areas correspond to municipalities from which zero women accessed support centers ($82$ municipalities).


```{r Log accesses plot, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Log-access rate" }

gridExtra::grid.arrange(ggy21, ggy22, ggy23, nrow = 3, ncol = 1)


```

## Covariates

Our target is explaining the number of accesses to support centers, $y$, defined at the municipality level, on the basis of a set of candidate known variables. Unfortunately, these data are only available for year 2021. $y$ is modelled with simple Poisson regression.

We have at disposal a number of candidate explanatory variables, which include the distance of a municipality from the closest support center and a set of variables measuring social vulnerability under different dimensions; these latter covariates are provided by the ISTAT.A more detailed description of these covariates is in [this excel metadata file](https://github.com/lcef97/CAV_Puglia/blob/main/Metadata/Indice_composito_fragilita_PUGLIA_2021.xlsx).

All covariates are scaled to have null mean and unit variance.

  -  $\mathrm{TEP}$, i.e. the distance of each municipality from the closest municipality hosting a support center. Distance is measured by road travel time in minutes (acronym TEP stays for Tempo Effettivo di Percorrenza, i.e. Actual Travel Time). Since to the best of our knowledge the list of active support centers changed between 2022 and 2023, we employ the list of centers active until 2022 for 2021-2022 data, and the list of centers active in 2023 for 2023 data.

  -  $\mathrm{AES}$, the distance from the closest infrastructural pole, always measured in travel time.
  -  $\mathrm{MFI}$, i.e. the decile of municipality vulnerability index.
  -  $\mathrm{PDI}$, i.e. the dependency index, i.e. population either $\leq 20$ or $\geq 65$ years over population in $[20 - 64]$ years.
  -  $\mathrm{ELL}$, i.e. the proportion of people aged $[25-54]$ with low education.
  -  $\mathrm{ERR}$, i.e. employment rate among people aged $[20-64]$.
  -  $\mathrm{PGR}$, i.e. population growth rate with respect to 2011.
  -  $\mathrm{UIS}$, i.e. the ventile of the density of local units of industry and services (where density is defined as the ratio between the counts of industrial units and population).
  -  $\mathrm{ELI}$, i.e. the ventile of employees in low productivity local units by sector for industry and services.

First, we visualise the correlations among these explanatory variables:

```{r correls, echo = F, warning = F, fig.height = 3, fig.cap = "Correlations in explanatory variables"}
ggplot2::ggplot(data = reshape2::melt(cor(X[,-1]))) +
  ggplot2::geom_tile(ggplot2::aes(
    x = .data$Var2, y = .data$Var1, fill = .data$value), color = "black", size = 3) +
  ggplot2::geom_text(ggplot2::aes(
    x = .data$Var2, y = .data$Var1, label = round(.data$value, 2))) +
  ggplot2::scale_fill_gradient2(
    low = "blue", mid = "white", high = "red", midpoint = 0) +
  ggplot2::theme_minimal()
```

We see the correlation between the two distances is very high ($0.72$), and so is the correlation between the fragility index decile and the density of productive units. 

In the first case, we drop the distance from the nearest infrastructural pole. In the latter we drop `MFI`, which is a combination of all covariates except for `TEP_th`, and is a weakly informative choice.


## Nonspatial regression

We regress the counts of accesses $y$ to support centers on the aforementioned explanatory variables. To estimate regression coefficients, all covariates are scaled to zero mean an unit variance. 

\begin{equation}
y_i \mid \eta_i \sim \mathrm{Poisson}(e^{\eta_i + P_i}) \quad \text{where} \quad
\eta_i = X_i^{\top} \beta
\label{eq:glm}
\end{equation}

Where $X$ are the covariate defined earlier, $\beta$ are covariate effects, and $P_i$ is the female population aged $\geq 15$ in municipality $i$.

To gain more insight on the role of all explanatory variables we show the posterior summaries of the full regression model

```{r glm full covs}
cav_glm_21 <- glm(N_ACC_21~ 1 +TEP_th_22 + ELI + PGR +
                 UIS + ELL + PDI + ER,
               family = "poisson",offset = log(nn21), data = dd_con)

cav_glm_22 <- glm(N_ACC_22~ 1 +TEP_th_22 + ELI + PGR +
                 UIS + ELL + PDI + ER,
               family = "poisson",offset = log(nn22), data = dd_con)

cav_glm_23 <- glm(N_ACC_23~ 1 +TEP_th_23 + ELI + PGR +
                 UIS + ELL + PDI + ER,
               family = "poisson",offset = log(nn23), data = dd_con)




```


```{r glm results, warning = FALSE, message = FALSE, echo = FALSE, results = "asis"}


betas.glm <- data.frame(
  Mean = c(summary(cav_glm_21)$coefficients[,1],
           summary(cav_glm_22)$coefficients[,1],
           summary(cav_glm_23)$coefficients[,1]),
  Sd = c(summary(cav_glm_21)$coefficients[,2],
           summary(cav_glm_22)$coefficients[,2],
           summary(cav_glm_23)$coefficients[,2]))  %>%
  dplyr::mutate( year = rep(c(2021, 2022, 2023), each = 8)) %>% 
  dplyr::mutate(Effect = rep(c("Intercept", "TEP", rownames(summary(cav_glm_21)$coefficients)[-c(1,2)]),3)) %>% 
  dplyr::relocate(.data$Effect, .before = 1)

betas.glm %<>%   tidyr::pivot_wider(
    id_cols = Effect,           # Use the unique coefficient names as row identifier
    names_from = year,               # Create new columns for each year
    values_from = c(Mean, Sd)        # Fill in values from Mean and Sd
  )

print(xtable::xtable(betas.glm, num.digits = 3), include.rowname = FALSE)
```


```{r INLA nosp, wargning = FALSE, echo = FALSE, message = FALSE, output = FALSE}
cav_nosp_inla <- inla(
  N_ACC ~ 0 + Intercept +TEP_th + ELI + PGR + UIS + ELL + PDI + ER,
  offset = log(nn),
  family = rep("poisson", 3), data =dd_list,
  control.inla = list(strategy = "laplace", int.strategy = "grid"),
  inla.mode = "classic",
  num.threads = 1, control.compute = list(internal.opt = F, cpo = T, waic = T, config = T), 
  verbose = T)

betas.nosp.inla  <- cav_nosp_inla$summary.fixed %>% 
  dplyr::select(.data$mean, .data$sd)  %>%
  dplyr::mutate( year = rep(c(2021, 2022, 2023), 8)) %>% 
  dplyr::mutate(Effect = rep(c("Intercept", "TEP", rownames(summary(cav_glm_21)$coefficients)[-c(1,2)]),each=3)) %>% 
  dplyr::relocate(.data$Effect, .before = 1) %>%  
  dplyr::rename(Mean = .data$mean, Sd = .data$sd) %>% 
  tidyr::pivot_wider( 
    id_cols = .data$Effect,            
    names_from = .data$year,                
    values_from = c(.data$Mean, .data$Sd))

#print(betas.nosp.inla, n.digits = 3)

```



  - `TEP_th_22`: The distance from the closest support center appears to play an important role. The easiest interpretation is that the physical distance represents a barrier to violence reporting. This is quite intuitive if we think of the material dynamics of reporting gender-based violence: one could reasonably expect violent men to prevent their partners to come out and report the violence suffered.
  
  - `ELI`: The (ventile of the distribution of the) share of employees in low productivity economic units is a clear indicator of (relative) economic underdevelopment. The most naive interpretation wuld be that in underdeveloped areas reporting gender violence is somewhat harder than in developed ones; however this relationship does not appear to be strong and is indeed negligible for 2021 and 2023 data.

  - `PGR`: The association with population growth rate is harder to interpret. This association is most likely influenced by several demographic instrumental variables we are not keeping into account and would indeed deserve a more dedicated focus. Only in 2022 does growth rate appear to have a significant association with AVCs accesses.
  
  - `UIS`: The (ventile of the distribution of the) density of production units has a somewhat ambiguous interpretation. From the one side, it has a strong negative relationship with the social frailty index. It should be therefore considered an indicator of economic development. Nevertheless, for 2022 data the regression coefficient bears the same negative sign as the incidence of low-productivity economic units; for 2023 data the association with AVCs accesses is positive instead. For 2021 data, this association appears not significantly different from zero. *Honestly I have no idea on how to interpret it*.

  - `ELL`: The association with the proportion of people with low educational level has negative sign and is high in absolute value. The interpretation seems quite easy: cultural development, in general, would encourage reporting violence.
  
  - `PDI`: The association with population dependency index does not seem significantly different from zero
  
  - `ER`: The association with employment rate is very strong and bears negative sign for 2021 and 2023 data.

## Spatial regression


We plot the log-residuals $\varepsilon$ of the GLM regression models, defined as $\varepsilon := \ln y_i - \ln P_i - \ln \hat{\eta}_i$ being $\hat{\eta}_i$ the expected predictor.


```{r glm residuals, echo = FALSE, warnings = FALSE, message = FALSE, output = FALSE}
L_block <- kronecker(diag(1, 3), Lapl_con)
A_constr <- t(pracma::nullspace(L_block))


resids_glm_21 <- log(dd_con$N_ACC_21) - log(cav_glm_21$fitted.values) - log(dd_con$nn21)
resids_glm_22 <- log(dd_con$N_ACC_22) - log(cav_glm_22$fitted.values) - log(dd_con$nn22)
resids_glm_23 <- log(dd_con$N_ACC_23) - log(cav_glm_23$fitted.values) - log(dd_con$nn23)

resids <- c(resids_glm_21, resids_glm_22, resids_glm_23)
rr <- range(resids[which(is.finite(resids))])


ggr21 <- dd_con %>% 
  dplyr::mutate(log_resids_2021 = resids_glm_21) %>% 
  ggplot2::ggplot() +
  ggplot2::geom_sf(ggplot2::aes(fill = .data$log_resids_2021))+
  ggplot2::labs("blank") +
  ggplot2::scale_fill_viridis_c(na.value = "white", direction = -1, limits = rr) +
  ggplot2::theme_classic()

ggr22 <- dd_con %>% 
  dplyr::mutate(log_resids_2022 = resids_glm_22) %>% 
  ggplot2::ggplot() +
  ggplot2::geom_sf(ggplot2::aes(fill = .data$log_resids_2022))+
  ggplot2::labs("blank") +
  ggplot2::scale_fill_viridis_c(na.value = "white", direction = -1, limits = rr) +
  ggplot2::theme_classic()


ggr23 <- dd_con %>% 
  dplyr::mutate(log_resids_2023 = resids_glm_23) %>% 
  ggplot2::ggplot() +
  ggplot2::geom_sf(ggplot2::aes(fill = .data$log_resids_2023))+
  ggplot2::labs("blank") +
  ggplot2::scale_fill_viridis_c(na.value = "white", direction = -1, limits = rr) +
  ggplot2::theme_classic()


#' Problem: cannot apply Moran test to infinite values!
#' Hence we need to only test autocorrelation across nonzero records.
#' This strongly limits the relevance of the test, if I have a better idea I'll implememnt it.
nonzero_21 <- which(dd_con$N_ACC_21 > 0)
nonzero_22 <- which(dd_con$N_ACC_22 > 0)
nonzero_23 <- which(dd_con$N_ACC_23 > 0)

spdep::set.ZeroPolicyOption(TRUE)
suppressWarnings(nb_nonzero_21 <- spdep::poly2nb(dd_con[nonzero_21, ]))
suppressWarnings(nb_nonzero_22 <- spdep::poly2nb(dd_con[nonzero_22, ]))
suppressWarnings(nb_nonzero_23 <- spdep::poly2nb(dd_con[nonzero_23, ]))


nonzero_singletons_21 <- which(unlist(lapply(nb_nonzero_21, function(X) X[1L]==0)))
nonzero_singletons_22 <- which(unlist(lapply(nb_nonzero_22, function(X) X[1L]==0)))
nonzero_singletons_23 <- which(unlist(lapply(nb_nonzero_23, function(X) X[1L]==0)))

if(length(nonzero_singletons_21 > 0)){
  nonzero_con_21 <- nonzero_21[-nonzero_singletons_21]
  } else nonzero_con_21 <- nonzero_21
nonzero_con_22 <- nonzero_22[-nonzero_singletons_22]
nonzero_con_23 <- nonzero_23[-nonzero_singletons_23]

nb_con_nonzero_21 <- spdep::poly2nb(dd_con[nonzero_con_21, ])
nb_con_nonzero_22 <- spdep::poly2nb(dd_con[nonzero_con_22, ])
nb_con_nonzero_23 <- spdep::poly2nb(dd_con[nonzero_con_23, ])





```
```{r Log residuals, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Log-residuals in GLM regression" }

gridExtra::grid.arrange(ggr21, ggr22, ggr23, nrow = 3, ncol = 1)


```


Residuals may exhibit spatial structure. To assess it, we employ the Moran and Geary tests. Since 

Please notice that log-residuals only take finite values across the municipalities whose female citizens have reported at least one case of violence in 2022.

Additionally, this set of municipalities may include some singletons, which we remove to assess the value of the Moran and Geary statistics. Thus, for each year we have defined the indexes set `nonzero_con` as the set of municipalities from which at least one case of gender-based violence has been reported, *and* which have at least one neighbouring municipality from which at least one case of gender-based violence was reported as well. For brevity, we only show the standardised $I$ values, which under the null hypothesis should be distributed as $N(0,1)$.

```{r moran residuals 2021}
unlist(spdep::moran.test(
  resids_glm_21[nonzero_con_21],
  listw = spdep::nb2listw(
    nb_con_nonzero_21))[c("statistic", "p.value")])
unlist(spdep::geary.test(
  resids_glm_21[nonzero_con_21],
  listw = spdep::nb2listw(
    nb_con_nonzero_21))[c("statistic", "p.value")])
```
```{r moran residuals 2022}
unlist(spdep::moran.test(
  resids_glm_22[nonzero_con_22],
  listw = spdep::nb2listw(
    nb_con_nonzero_22))[c("statitic", "p.value")])
unlist(spdep::geary.test(
  resids_glm_22[nonzero_con_22],
  listw = spdep::nb2listw(
    nb_con_nonzero_22))[c("statistic", "p.value")])

```
```{r moran residuals 2023}
unlist(spdep::moran.test(
  resids_glm_23[nonzero_con_23],
  listw = spdep::nb2listw(
    nb_con_nonzero_23))[c("statistic", "p.value")])
unlist(spdep::geary.test(
  resids_glm_23[nonzero_con_23],
  listw = spdep::nb2listw(
    nb_con_nonzero_23))[c("statistic", "p.value")])

```



In all these three cases, we find evidence for spatial autocorrelation. However, we must stress out this result does not refer to all the regional territory, but only to a subset of all municipalities.


Based on the autocorrelation evidence, though it has only been assessed for a subset of all municipalities, we try implementing some simple spatial models by adding a conditionally autoregressive latent effect, say $z$, to the linear predictor


\begin{equation}
\eta_i = X_i^{\top} \beta + z_i 
\label{eq:mspat}
\end{equation}


We test a total of three models, all of which have a prior distribution depending on the spatial structure of the underlying graph, in this case the Apulia region. 

We describe the spatial structure starting from municipality neighbourhood, and introduce the neighbourhood matrix $W$, whose generic element $w_{ij}$ takes value $1$ if municipalities $i$ and $j$ are neighbours and $0$ otherwise. For each $i \in [1,n]$, $d_i:= \sum_{j=1}^n w_{ij}$ is the number of neigbours of $i$-th municipality. Plase notice we have have $n = 256$.

For all models, we define $\Lambda$ as the precision parameter of the latent effect, and assign it a Wishart prior.

Spatial models are computed by approximating the marginal posteriors of interest via the Integrated Nested Laplace Approximation (INLA), adopting the novel Variational Bayes Approach [@INLAVB].

Priors for spatial effects have been defined using the \texttt{INLAMSM} \texttt{R} package [@INLAMSM].


#### ICAR model

The Intrinsic CAR model is the simplest formulation among spatial autoregressive models. The conditional distribution of each value $z_i \mid z_{-i}$ is:

\begin{equation}
z_i \mid z_{-i} \sim N \left( \sum_{j=1}^n \frac{w_{ij}}{d_i} z_j \,, \frac{1}{d_i} \Lambda^{-1} \right)
\label{eq:icar_loc}
\end{equation}

Since the joint distribution of $z$ is improper, a sum-to-zero constraint is required for identifiability.



```{r inla icar, echo = FALSE, message = FALSE, warning = FALSE}

cav_IMCAR_inla <- inla(
  N_ACC ~ 0 + Intercept +TEP_th + ELI + PGR + UIS + ELL + PDI + ER+ 
    f(ID, model = inla.IMCAR.model(k = 3, W = W_con), extraconstr = list(
      A = A_constr, e = c(rep(0, nrow(A_constr))))),
  offset = log(nn),
  family = rep("poisson", 3), data =dd_list,
  #inla.mode = "classic", control.inla = list(strategy = "laplace", int.strategy = "grid"),
  num.threads = 1, control.compute = list(internal.opt = F, cpo = T, waic = T, config = T), 
  verbose = T)
```

#### PCAR model

The intrinsic autoregressive model is relatively simple to interpret and to implement, 
while also requiring the minimum number of additional parameter (either the scale or the precision).


The drawback, however, is that we implicitly assume a deterministic spatial autocorrelation coefficient equal to 1.
When the autocorrelation is weak, setting an ICAR prior may be a form of misspecification.

A generalisation of this model is the PCAR (proper CAR), which introduces an autocorrelation parameter $\alpha$:

\begin{equation}
z_i \mid z_{-i} \sim N \left( \sum_{j=1}^n \alpha \frac{w_{ij}}{d_i} z_j \,, \frac{}{d_i} \Lambda^{-1}\right)
\label{eq:pcar_loc}
\end{equation}


```{r pcar run, message = FALSE, echo = FALSE, output = FALSE}

cav_PMCAR_inla <- inla(
  N_ACC ~ 1 +TEP_th + ELI + PGR + UIS + ELL + PDI + ER+ 
    f(ID, model = inla.MCAR.model(k = 3, W = W_con,  alpha.min = 0,alpha.max = 1)),
  offset = log(nn),
  family = rep("poisson", 3), data =dd_list,
  #control.fixed = list(prec = list(Intercept1 = 0, Intercept2 = 0, Intercept3 = 0)),
  #inla.mode = "classic", control.inla = list(strategy = "laplace", int.strategy = "grid"),
  num.threads = 1, control.compute = list(internal.opt = F, cpo = T, waic = T, config = T), 
  verbose = T)

```

We show the posterior summary for the autocorrelation coefficient. 

```{r alfa, echo = FALSE, fig.height = 3, fig.cap = "Posterior marginal of the PCAR autocorrelation parameter"}
alpha_marg_pcar <- inla.tmarginal(
  fun = function(X) 1/(1 + exp(-X)),
  marginal = cav_PMCAR_inla$marginals.hyperpar[[1]])

plot(alpha_marg_pcar, type = 'l', xlab = expression(alpha), ylab = expression(pi (alpha ~ "|" ~ y)  ))

unlist(inla.zmarginal(alpha_marg_pcar, silent = TRUE))
```
 
 The credible interval for $\alpha$ is quite pushed towards unity, denoting the model estimates a strong spatial autocorrelation. 
 
#### Leroux model

As an alternative to take into account both structured and unstructured latent effects, we also test the Leroux autoregressive model [@Leroux]. In this case, the local prior for $z_i$ is

\begin{equation}
z_i \mid z_{-i} \sim N \left( \sum_{j=1}^n \frac{\xi w_{ij}}{1 - \xi + \xi d_i}   z_j \,, \Lambda^{-1} \frac{1}{1 - \xi + \xi d_i}\right)
\label{eq:leroux_loc}
\end{equation}

Where $\xi \in [0, 1]$ is the mixing parameter. A more interesting representation of the Leroux model is the joint prior
$$
z \mid \Lambda, \xi \sim N(0, [ \Lambda \otimes (\xi L + (1-\xi)I)]^{-1})
$$
where $L := D-W$ is the graph Laplacian matrix, $W$ is the neighbourhood matrix and $D$ is the corresponding degree matrix. We can clearly see how the mixing parameter allocates variability between two precision components, i.e. the Laplacian matrix for the spatial part and the the identity matrix for the noise.

The drawback of this model is the scarce interpretability with respect to more sophisticated ones like the BYM, but the multivariate framework complicates the definition of the BYM model (and it does not seem to be possible to reparametrise it in such a way to have a sparse precision, hence computations are unfeasible) and hampers the use of PC priors, which would have indeed been a useful tool to prevent overfitting.


```{r INLA Leroux, echo = FALSE}

inla.rgeneric.MLCAR.model <- 
  function (cmd = c("graph", "Q", "mu", "initial", "log.norm.const", 
                    "log.prior", "quit"), theta = NULL) 
  {
    interpret.theta <- function() {
      alpha <-  1/(1 + exp(-theta[1L]))
      mprec <- sapply(theta[as.integer(2:(k + 1))], function(x) {
        exp(x)
      })
      corre <- sapply(theta[as.integer(-(1:(k + 1)))], function(x) {
        (2 * exp(x))/(1 + exp(x)) - 1
      })
      param <- c(alpha, mprec, corre)
      n <- (k - 1) * k/2
      M <- diag(1, k)
      M[lower.tri(M)] <- param[k + 2:(n + 1)]
      M[upper.tri(M)] <- t(M)[upper.tri(M)]
      st.dev <- 1/sqrt(param[2:(k + 1)])
      st.dev.mat <- matrix(st.dev, ncol = 1) %*% matrix(st.dev, 
                                                        nrow = 1)
      M <- M * st.dev.mat
      PREC <- solve(M)
      return(list(alpha = alpha, param = param, VACOV = M, 
                  PREC = PREC))
    }
    graph <- function() {
      PREC <- matrix(1, ncol = k, nrow = k)
      G <- kronecker(PREC, Matrix::Diagonal(nrow(W), 1) + 
                       W)
      return(G)
    }
    Q <- function() {
      param <- interpret.theta()
      Lapl <- Matrix::Diagonal(nrow(W), apply(W, 1, sum)) -  W
      #Sigma.u <- MASS::ginv(as.matrix(Lapl))
      #Sigma <- param$alpha * Sigma.u + (1-param$alpha)*diag(1, nrow(W))
      R <- param$alpha*Lapl + (1-param$alpha)*diag(1, nrow(W))
      Q <- kronecker(param$PREC, R)
      return(Q)
    }
    mu <- function() {
      return(numeric(0))
    }
    log.norm.const <- function() {
      val <- numeric(0)
      return(val)
    }
    log.prior <- function() {
      param <- interpret.theta()
      # Uniform prior on \lambda
      # val <- -theta[1L] - 2 * log(1 + exp(-theta[1L]))
      # Normal prior on \lambda, in analogy with the univariate default case
      val <- log(dnorm(theta[1L], mean = 0, sd = sqrt(1/0.45))) 
      val <- val + log(MCMCpack::dwish(
        W = param$PREC, v = k, S = diag(rep(1, k)))) +
        sum(theta[as.integer(2:(k + 1))]) + 
        sum(log(2) + theta[-as.integer(1:(k + 1))] - 
              2 * log(1 + exp(theta[-as.integer(1:(k + 1))])))
      return(val)
    }
    initial <- function() {
      return(c(0, rep(0, k), rep(0, (k * (k - 1)/2))))
    }
    quit <- function() {
      return(invisible())
    }
    if (as.integer(R.version$major) > 3) {
      if (!length(theta)) 
        theta = initial()
    }
    else {
      if (is.null(theta)) {
        theta <- initial()
      }
    }
    val <- do.call(match.arg(cmd), args = list())
    return(val)
  }

inla.MLCAR.model <- function(...) INLA::inla.rgeneric.define(inla.rgeneric.MLCAR.model, ...)

cav_MLCAR_inla <- inla(
  N_ACC ~ 1 +TEP_th + ELI + PGR + UIS + ELL + PDI + ER+ 
    f(ID, model = inla.MLCAR.model(k = 3, W = W_con)),
  offset = log(nn),
  family = rep("poisson", 3), data =dd_list,
  #control.fixed = list(prec = list(Intercept1 = 0, Intercept2 = 0, Intercept3 = 0)),
  #inla.mode = "classic", control.inla = list(strategy = "laplace", int.strategy = "grid"),
  num.threads = 1, control.compute = list(internal.opt = F, cpo = T, waic = T, config = T), 
  verbose = T)

```

#### BYM model


Another popular model to control for both spatial autocorrelation and random noise is the Besag, York and MolliÃ© model. 
We employ a simplified formulation, with a unique mixing parameter for all three years. Under this model the latent effect is defined as: 

\begin{equation}
z = \sqrt{\phi} u M + \sqrt{1-\phi} vM
\label{eq:bym}
\end{equation}



Where: 
  - $u \sim N(0, I_p \otimes L_\mathrm{scaled})$ is an independent multivariate ICAR process whose precision matrix is scaled in order that the geometric mean of marginal variances (i.e. the diagonal entries of $R^{+}$) is one
  - $v \sim N(0, I_p \otimes I_n)$ is a Standard Normal variable
  - $M$ is a positive definite matrix such that $M'M = \Lambda^{-1}$. It is not necessarily the Cholesky factor of the scale parameter; in fact, a convenient but not unique way to define it may be $M = D^{-\frac{1}{2}}E^{\top}$ where $E$ and $D$ are the eigenvector and eigenvalues matrices of $\Lambda$ [@Urdangarin]. 

  - $\phi \in [0,1]$ is the mixing parameter.
We assign a Uniform prior on $\phi$ (but the PC-prior would be a more rigorous choice) and the usual Wishart prior to $\Lambda$.



The BYM model intruduces a mixing parameter as the LCAR does, but improves its interpretability by allowing to scale the ICAR and IID components. 

We use a rather primitive BYM definition here. It could be improved in many directions: either defining a PC prior on the mixing parameter and employing a more elegant parametrisation allowing for sparse precision [@BYM2], or relaxing the naive hypothesis of a unique mixing parameter implementing an M-model.

However, even this rudimentary formulation allows to interpret the mixing parameter more clearly.

```{r BYM, echo = FALSE, warning = FALSE, message = FALSE, output = FALSE}
inla.rgeneric.MBYM.dense <- 
  function (cmd = c("graph", "Q", "mu", "initial", "log.norm.const", 
                    "log.prior", "quit"), theta = NULL ) {
    envir <- parent.env(environment())
    if(!exists("cache.done", envir=envir)){
      starttime.scale <- Sys.time()
      #' Unscaled Laplacian matrix (marginal precision of u_1, u_2 ... u_k)
      L_unscaled <- Matrix::Diagonal(nrow(W), rowSums(W)) -  W
      L_unscaled_block <- kronecker(diag(1,k), L_unscaled)
      A_constr <- t(pracma::nullspace(as.matrix(L_unscaled_block)))
      scaleQ <- INLA:::inla.scale.model.internal(
        L_unscaled_block, constr = list(A = A_constr, e = rep(0, nrow(A_constr))))
      #' Block Laplacian, i.e. precision of U = I_k \otimes L
      n <- nrow(W)
      L <- scaleQ$Q[c(1:n), c(1:n)]
      Sigma.u <- MASS::ginv(as.matrix(L))
      if(PC == TRUE){
        #' Eigenvalues of the SCALED Laplacian, sufficient for trace and determinant entering the KLD
        L_eigen_scaled <- eigen(scaleQ$Q)$values
        #' PC prior on mixing parameter - definition should be fine.
        log.dpc.phi.bym <- INLA:::inla.pc.bym.phi(eigenvalues = L_eigen_scaled, 
                                                  marginal.variances = scaleQ$var,
                                                  rankdef = nrow(A_constr),
                                                  u = 0.5, alpha = 2/3)
        assign("log.dpc.phi.bym", log.dpc.phi.bym, envir = envir)
        
      }
      endtime.scale <- Sys.time()
      cat("Time needed for scaling Laplacian matrix: ",
          round(difftime(endtime.scale, starttime.scale), 3), " seconds \n")
      assign("L", L, envir = envir)
      assign("Sigma.u", Sigma.u, envir = envir)
      assign("cache.done", TRUE, envir = envir)
    }
    interpret.theta <- function() {
      alpha <-  1/(1 + exp(-theta[1L]))
      mprec <- sapply(theta[as.integer(2:(k + 1))], function(x) {
        exp(x)
      })
      corre <- sapply(theta[as.integer(-(1:(k + 1)))], function(x) {
        (2 * exp(x))/(1 + exp(x)) - 1
      })
      param <- c(alpha, mprec, corre)
      n <- (k - 1) * k/2
      M <- diag(1, k)
      M[lower.tri(M)] <- param[k + 2:(n + 1)]
      M[upper.tri(M)] <- t(M)[upper.tri(M)]
      st.dev <- 1/sqrt(param[2:(k + 1)])
      st.dev.mat <- matrix(st.dev, ncol = 1) %*% matrix(st.dev, 
                                                        nrow = 1)
      M <- M * st.dev.mat
      PREC <- solve(M)
      return(list(alpha = alpha, param = param, VACOV = M, 
                  PREC = PREC))
    }
    graph <- function() {
      PREC <- matrix(1, ncol = k, nrow = k)
      G <- kronecker(PREC, Matrix::Diagonal(nrow(W), 1) + 
                       W)
      return(G)
    }
    Q <- function() {
      param <- interpret.theta()
      #' Weighted average of ICAR and IID variables: variance is the sum of variances.
      #' Precision here defined as inverse variance. Not
      #' the best way to do it; still using sparse
      #' parametrisation requires a latent effect of length 2*np
      Sigma <- param$alpha * Sigma.u + (1-param$alpha)*diag(1, nrow(W))
      Q <- kronecker(param$PREC, solve(Sigma))
      return(Q)
    }
    mu <- function() {
      return(numeric(0))
    }
    log.norm.const <- function() {
      val <- numeric(0)
      return(val)
    }
    log.prior <- function() {
      param <- interpret.theta()
      if(PC == TRUE){
        #' PC prior implementation
        val <- log.dpc.phi.bym(param$phi)- theta[1L] - 2 * log(1 + exp(-theta[1L]))
      } else {
        #' Uniform prior
        val <- -theta[1L] - 2 * log(1 + exp(-theta[1L]))
      }
      #' Whishart prior on precision (inverse scale)
      val <- val + log(MCMCpack::dwish(W = param$PREC, v = k,
                                       S = diag(rep(1, k)))) +
        #' This for the change of variable
        #' (code from INLAMSM)
        sum(theta[as.integer(2:(k +  1))]) +
        sum(log(2) + theta[-as.integer(1:(k + 1))] - 2 * log(1 + exp(theta[-as.integer(1:(k + 1))])))
      return(val)
    }
    initial <- function() {
      if(!exists("init", envir = envir)){
        return(c(0, rep(0, k), rep(0, (k * (k - 1)/2))))
      } else{
        return(init)
      }
    }
    quit <- function() {
      return(invisible())
    }
    if (as.integer(R.version$major) > 3) {
      if (!length(theta)) 
        theta = initial()
    }
    else {
      if (is.null(theta)) {
        theta <- initial()
      }
    }
    val <- do.call(match.arg(cmd), args = list())
    return(val)
  }


inla.MBYM.dense <- function(...) INLA::inla.rgeneric.define(inla.rgeneric.MBYM.dense, ...)

cav_MBYM_inla <- inla(
  N_ACC ~ 0 + Intercept +TEP_th + ELI + PGR + UIS + ELL + PDI + ER+ 
    f(ID, model = inla.MBYM.dense(k = 3, W = W_con, 
                                  PC = FALSE),
      extraconstr = list(A = A_constr, e = rep(0, 3))) ,
  offset = log(nn),
  family = rep("poisson", 3), data =dd_list,
  num.threads = 1, control.compute = list(internal.opt = F, cpo = T, waic = T, config = T), 
  verbose = T)


```

#### Spatiotemporal model

A different perspective than multivariate modelling would be spatiotemporal modelling. 

In this case, we would assume the DGP of the linear predictor to be of this form:
\begin{equation}
\eta_{it} = X_{it}^{\top} \beta_t + z_{i} + \delta_{it}
\label{eq:st}
\end{equation}

For brevity, we immediately assume $z$ is a BYM process, hence $z = \sigma \left( \sqrt(\phi) u +\sqrt(1-\phi)v \right)$, where $u$ is an ICAR field with typical variance $=1$ and $v$ is a Standard iid Gaussian field. 

The term $\delta_{it}$ controls for spatio-temporal interaction. To our aims, two spatiotemporal interactions are relevant:
  - Type I interaction, i.e. interaction between the temporal effect (assumed IID) and $v$
  - Type III interaction, i.e. interaction between the temporal effect (assumed IID) and $u$.

Since the purely temporal effect is negligible we do not bother about defining priors on it different than the default.

We start fitting the type-I model. Type-III apparently has some misspecifications needing to be fixed. 

I don't know if this is correct, but a PC-prior is set on $\delta_{it}$, though its definition relies on the assumption that $z$ is fixed.

```{r spatiotemporal models, output = F, warning = F, message = F}
cav_STbym_i1_INLA <- inla(N_ACC ~ 1 +TEP_th + ELI + PGR + UIS + ELL + PDI + ER +
                            f(Area, model = "bym2", graph = W_con,  scale.model = T, 
                              hyper = list(prec = list(prior = "pc.prec", param = c(1.5, 0.01))))+
                            f(Year, model = "iid") + 
                            f(ID, model = "iid",
                              hyper = list(prec = list(prior = "pc.prec", param = c(1.5, 0.01)))),
                            family = c("poisson", "poisson", "poisson"), offset = log(nn), data =dd_list_st,
                            num.threads = 1, control.compute = 
                              list(internal.opt = F, cpo = T, waic = T), 
                            #inla.mode = "classic", control.inla = list(strategy = "laplace", int.strategy = "grid"),
                            control.predictor = list(compute = T),
                            verbose = T)




```


#### Model assessment 

We briefly compare the three models in scope through the WAIC [@GelmanWAIC]:

```{r WAICs table, echo = F, results = "asis"}

WAICS <- tibble::tibble(
  Model = c("Null", "ICAR", "PCAR", "Leroux", "BYM", "ST_I"),
  WAIC = round(c(
    cav_nosp_inla$waic$waic,  cav_IMCAR_inla$waic$waic,  cav_PMCAR_inla$waic$waic, 
    cav_MLCAR_inla$waic$waic, cav_MBYM_inla$waic$waic, cav_STbym_i1_INLA$waic$waic),3),
  Eff_params = round(c(
    cav_nosp_inla$waic$p.eff,   cav_IMCAR_inla$waic$p.eff,  cav_PMCAR_inla$waic$p.eff,
     cav_MLCAR_inla$waic$p.eff, cav_MBYM_inla$waic$p.eff, cav_STbym_i1_INLA$waic$p.eff),3))

print(xtable::xtable(WAICS),include.rowname = F)

```

As we can see, adding a spatial model is an improving element, not a waste of complexity.



## Preliminary findings

Here we show some posterior summaries for $\beta$ under the BYM model.

<!--
Since the latent effect has a proper distribution and does not require sum to zero constraints, using year-specific intercepts would cause confounding between intercepts and latent effects, leading to system crashing and thus failure to estimate the model. As a consequence, we used the same intercept for the three different years, allowing the latent effect to have *a posteriori* an additive year-specific shift.
-->

```{r betas bym, echo = FALSE, results = "asis"}
betas.MBYM.inla   <- cav_MBYM_inla$summary.fixed %>% 
  dplyr::select(.data$mean, .data$sd)  %>%
  dplyr::mutate( year = rep(c(2021, 2022, 2023), 8)) %>% 
  dplyr::mutate(Effect = rep(c("Intercept", "TEP",
                               rownames(summary(cav_glm_21)$coefficients)[-c(1,2)]),each=3)) %>% 
  dplyr::relocate(.data$Effect, .before = 1) %>%  
  dplyr::rename(Mean = .data$mean, Sd = .data$sd) %>% 
  tidyr::pivot_wider( 
    id_cols = .data$Effect,            
    names_from = .data$year,                
    values_from = c(.data$Mean, .data$Sd))

#betas.MBYM.inla <- dplyr::bind_rows(
#  data.frame(Effect = "Intercept", 
#             Mean_2021 = cav_MBYM_inla$summary.fixed$mean[1],
#             Sd_2021 = cav_MBYM_inla$summary.fixed$sd[1],
#             Mean_2022 = cav_MBYM_inla$summary.fixed$mean[1],
#             Sd_2022 = cav_MBYM_inla$summary.fixed$sd[1],
#             Mean_2023 = cav_MBYM_inla$summary.fixed$mean[1],
#             Sd_2023 = cav_MBYM_inla$summary.fixed$sd[1]),
 # betas.MBYM.inla.noint) 

print(xtable::xtable(betas.MBYM.inla, num.digits = 3), include.rowname = FALSE)

```


Estimations of $\beta$ differ slightly from the nonspatial model. For all variables, credibility intervals are wider due to increased uncertainty. 

  - `TEP_th_22` The effect of the distance from the closest support center remains similar in mean and the interpretation is not altered.

  - `ELI`: The effect of the incidence of low-productivity economic units is utterly negligible

  - `PGR`: The association with population growth rate can only be considered barely significant for 2022 data
  
  - `UIS`: The association with the density of productive units is negligible for 2021 and 2022 data, and can be considered slightly significant for 2023, bearing positive sign.  
  
  - `ELL`: The association with the incidence of low education levels, is even higher in mean than under the nonspatial model. We interpret this result as a strong *potential* impact of education on the chance that gender violence is reported
  
  - `PDI`: The effect of structural dependency index is utterly negligible, as for the GLM. 
  
  - `ER`: The effect associated with employment rate is increased for 2021 data,  more than doubled for 2022 data, and slightly increased for 2023 data. How to interpret this finding? Employment rate is clearly an indicator of economic development, hence the easiest interpretation is that - as it was with `ELI` under the nonspatial model - in more developed areas there is a higher chance that gender violence is reported.
  
  
We show the expected latent effects
```{r zhat mbym, echo = FALSE, message = FALSE, warning = FALSE, output = FALSE}

zbound <- range(cav_MBYM_inla$summary.random$ID$mean)

ggzhat21 <- dd_con %>% 
  dplyr::mutate(zhat_21 = cav_MBYM_inla$summary.random$ID$mean[c(1:n)]) %>% 
  ggplot2::ggplot() +
  ggplot2::geom_sf(ggplot2::aes(fill = .data$zhat_21))+
  ggplot2::scale_fill_viridis_c(na.value = "white", direction = -1, limits = zbound) +
  ggplot2::theme_classic()

ggzhat22 <- dd_con %>% 
  dplyr::mutate(zhat_22 = cav_MBYM_inla$summary.random$ID$mean[c(n+(1:n))]) %>% 
  ggplot2::ggplot() +
  ggplot2::geom_sf(ggplot2::aes(fill = .data$zhat_22))+
  ggplot2::scale_fill_viridis_c(na.value = "white", direction = -1, limits = zbound) +
  ggplot2::theme_classic()


ggzhat23 <- dd_con %>% 
  dplyr::mutate(zhat_23 = cav_MBYM_inla$summary.random$ID$mean[c((2*n)+(1:n))]) %>% 
  ggplot2::ggplot() +
  ggplot2::geom_sf(ggplot2::aes(fill = .data$zhat_23))+
  ggplot2::scale_fill_viridis_c(na.value = "white", direction = -1, limits = zbound) +
  ggplot2::theme_classic()

```

```{r zhat plot, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Expected latent effect using the Leroux model" }

gridExtra::grid.arrange(ggzhat21, ggzhat22, ggzhat23, nrow = 3, ncol = 1)


```
As we can see, the behaviour of 2023 data is quite puzzling, since high values of the latent effect are observed in areas such as the Northern province of Foggia, which was previously characterised by low accesses.

We additionally show the fitted values of the counts of accesses, rounded to the closest integer.

```{r yhat mbym, echo = FALSE, message = FALSE, warning = FALSE, output = FALSE}

ybound <- c(0, (max(cav_MBYM_inla$summary.fitted.values$mean)+1))

ggyhat21 <- dd_con %>% 
  dplyr::mutate(yhat_21 = round(cav_MBYM_inla$summary.fitted.values$mean[c(1:n)],0)) %>% 
  ggplot2::ggplot() +
  ggplot2::geom_sf(ggplot2::aes(fill = .data$yhat_21))+
  ggplot2::scale_fill_viridis_c(na.value = "white", direction = -1, limits = ybound) +
  ggplot2::theme_classic()

ggyhat22 <- dd_con %>% 
  dplyr::mutate(yhat_22 = round(cav_MBYM_inla$summary.fitted.values$mean[c(n+(1:n))], 0)) %>% 
  ggplot2::ggplot() +
  ggplot2::geom_sf(ggplot2::aes(fill = .data$yhat_22))+
  ggplot2::scale_fill_viridis_c(na.value = "white", direction = -1, limits = ybound) +
  ggplot2::theme_classic()


ggyhat23 <- dd_con %>% 
  dplyr::mutate(yhat_23 = round(cav_MBYM_inla$summary.fitted.values$mean[c((2*n)+(1:n))],0)) %>% 
  ggplot2::ggplot() +
  ggplot2::geom_sf(ggplot2::aes(fill = .data$yhat_23))+
  ggplot2::scale_fill_viridis_c(na.value = "white", direction = -1, limits = ybound) +
  ggplot2::theme_classic()

```

```{r yhat plot, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Fitted values using the Leroux model" }

gridExtra::grid.arrange(ggyhat21, ggyhat22, ggyhat23, nrow = 3, ncol = 1)


```






We show the posterior median of the marginal variances of $z$, i.e. $\Lambda^{-1}$:
```{r Sigma LCAR, echo = FALSE, eval = FALSE, message = FALSE, warning = FALSE}
Sigma_diag_ls <- sapply(c(2,3,4), function(i){
  inla.zmarginal(inla.tmarginal(fun = function(x) {
    exp(-x)
    },  marginal =  cav_MBYM_inla$marginals.hyperpar[[i]]),
    silent = T)
})


Sigma_diag_mat <- rbind(
  unlist(Sigma_diag_ls[c(1,2,3,5,7)]),
  unlist(Sigma_diag_ls[7+c(1,2,3,5,7)]),
  unlist(Sigma_diag_ls[14+c(1,2,3,5,7)])
)

Sigma_diag_df <- as.data.frame(Sigma_diag_mat) %>% 
  dplyr::mutate(Year = c("2021", c("2022"), c("2023"))) %>% 
  dplyr::relocate(.data$Year, .before = 1)
names(Sigma_diag_df)[-1] <- c("Mean", "Sd", "Q0.025", "Median", "Q0.975")

print(xtable::xtable(Sigma_diag_df, digits = 3), include.rowname = F)

#Sigma_offdiag <- sapply(c(5, 6, 7), function(i){
#  inla.zmarginal(inla.tmarginal(fun = function(x) {
#    (2 * exp(x))/(1 + exp(x)) - 1
#    }, marginal =  cav_MBYM_inla$marginals.hyperpar[[i]]),
#    silent = T)$quant0.5
#})

#Sigma <- Matrix::Diagonal(x = Sigma_diagonal)
#Sigma[lower.tri(Sigma)] <- Sigma_offdiag
#Sigma[upper.tri(Sigma)] <- t(Sigma[lower.tri(Sigma)])
#print(Sigma)

```
For more insight on the dependence structure between the three years, posterior summaries for correlations are shown in the following. In terms of point estimates, the only high correlation appears that of 2021-2022.
```{r covariances, message = FALSE, warning = FALSE, echo = FALSE, results = "asis"}

covars_ls <- sapply(c(5, 6, 7), function(i){
  inla.zmarginal(inla.tmarginal(fun = function(x) {
    (2 * exp(x))/(1 + exp(x)) - 1
    }, marginal =  cav_PMCAR_inla$marginals.hyperpar[[i]]),
    silent = T)
})

cov_mat <- rbind(
  unlist(covars_ls[c(1,2,3,5,7)]),
  unlist(covars_ls[7+c(1,2,3,5,7)]),
  unlist(covars_ls[14+c(1,2,3,5,7)])
)
cov_df <- as.data.frame(cov_mat) %>% 
  dplyr::mutate(Years = c("2021-22", c("2021-23"), c("2022-23"))) %>% 
  dplyr::relocate(.data$Years, .before = 1)
names(cov_df)[-1] <- c("Mean", "Sd", "Q0.025", "Median", "Q0.975")

print(xtable::xtable(cov_df, digits = 3), include.rowname = F)


```

  
Lastly, we have a look at the mixing parameter under both the Leroux and BYM models.


```{r mixing leroux, echo = FALSE, fig.height = 3, fig.cap = "Posterior marginal of the LCAR mixing parameter"}
xi_marg_lcar <- inla.tmarginal(
  fun = function(X) 1/(1 + exp(-X)),
  marginal = cav_MLCAR_inla$marginals.hyperpar[[1]])

plot(xi_marg_lcar, type = 'l', xlab = expression(xi), ylab = expression(pi (xi ~ "|" ~ y)  ))

unlist(inla.zmarginal(xi_marg_lcar, silent = TRUE))
```

As we see, the Leroux mixing parameter is not particularly high, but interpreting it properly is hampered by the difficulty in scaling the precision matrix, which is a drawback of non-intrinsic models. 

```{r mixing bym, echo = FALSE, fig.height = 3, fig.cap = "Posterior marginal of the BYM mixing parameter"}
phi_marg_bym <- inla.tmarginal(
  fun = function(X) 1/(1 + exp(-X)),
  marginal = cav_MBYM_inla$marginals.hyperpar[[1]])

plot(phi_marg_bym, type = 'l', xlab = expression(xi), ylab = expression(pi (phi ~ "|" ~ y)  ))

unlist(inla.zmarginal(phi_marg_bym, silent = TRUE))
```

```{r hyperpars mute, eval = FALSE, echo = FALSE}


sigma_ls <- sapply(c(2,3,4), function (i){
    inla.zmarginal(inla.tmarginal(fun = function(x) {
    exp(-x)
    }, marginal =  cav_MBYM_inla$marginals.hyperpar[[i]]),
    silent = T)
})

phi_ls <- inla.zmarginal(inla.tmarginal(
  fun = function(X) 1/(1 + exp(-X)),
  marginal = cav_MBYM_inla$marginals.hyperpar[[1]]), silent = T)


hyperpar_mat <- rbind(
  unlist(phi_ls[c(1,3,5,7)]),
  unlist(sigma_ls[c(1,3,5,7)]),
  unlist(sigma_ls[7+c(1,3,5,7)]),
  unlist(sigma_ls[14+c(1,3,5,7)]),
  unlist(covars_ls[c(1,3,5,7)]),
  unlist(covars_ls[7+c(1,3,5,7)]),
  unlist(covars_ls[14+c(1,3,5,7)]))

hyperpar_df <- as.data.frame(hyperpar_mat) %>% 
  dplyr::mutate(Var = c("Phi",
                        "Sigma_21", "Sigma_22", "Sigma_23",
                        "rho_2122", "rho_2123", "rho_2223")) %>% 
  dplyr::relocate(.data$Var, .before = 1)
names(hyperpar_df)[-1] <- c("Mean", "Q0.025", "Median", "Q0.975")

print(xtable::xtable(hyperpar_df, digits = 3), include.rowname = F)


```

## Appendices

#### R code to fit the LCAR

The \texttt{R} code for the multivariate Leroux CAR model is shown. It is derived from the code for the PCAR in the     texttt{R} package \texttt{INLAMSM} [@INLAMSM].

A Gaussian prior is set on the logit of the mixing parameter for analogy with the default prior used in the univariate version (model \texttt{"besagproper"} in \texttt{R-INLA})

```{r INLA Leroux silent, eval = FALSE}

inla.rgeneric.MLCAR.model <- 
  function (cmd = c("graph", "Q", "mu", "initial", "log.norm.const", 
                    "log.prior", "quit"), theta = NULL) 
  {
    interpret.theta <- function() {
      alpha <-  1/(1 + exp(-theta[1L]))
      mprec <- sapply(theta[as.integer(2:(k + 1))], function(x) {
        exp(x)
      })
      corre <- sapply(theta[as.integer(-(1:(k + 1)))], function(x) {
        (2 * exp(x))/(1 + exp(x)) - 1
      })
      param <- c(alpha, mprec, corre)
      n <- (k - 1) * k/2
      M <- diag(1, k)
      M[lower.tri(M)] <- param[k + 2:(n + 1)]
      M[upper.tri(M)] <- t(M)[upper.tri(M)]
      st.dev <- 1/sqrt(param[2:(k + 1)])
      st.dev.mat <- matrix(st.dev, ncol = 1) %*% matrix(st.dev, 
                                                        nrow = 1)
      M <- M * st.dev.mat
      PREC <- solve(M)
      return(list(alpha = alpha, param = param, VACOV = M, 
                  PREC = PREC))
    }
    graph <- function() {
      PREC <- matrix(1, ncol = k, nrow = k)
      G <- kronecker(PREC, Matrix::Diagonal(nrow(W), 1) + 
                       W)
      return(G)
    }
    Q <- function() {
      param <- interpret.theta()
      Lapl <- Matrix::Diagonal(nrow(W), apply(W, 1, sum)) -  W
      #Sigma.u <- MASS::ginv(as.matrix(Lapl))
      #Sigma <- param$alpha * Sigma.u + (1-param$alpha)*diag(1, nrow(W))
      R <- param$alpha*Lapl + (1-param$alpha)*diag(1, nrow(W))
      Q <- kronecker(param$PREC, R)
      return(Q)
    }
    mu <- function() {
      return(numeric(0))
    }
    log.norm.const <- function() {
      val <- numeric(0)
      return(val)
    }
    log.prior <- function() {
      param <- interpret.theta()
      # Uniform prior on \lambda
      # val <- -theta[1L] - 2 * log(1 + exp(-theta[1L]))
      # Normal prior on logit lambda, in analogy with the univariate default case
      val <- log(dnorm(theta[1L], mean = 0, sd = sqrt(1/0.45))) 
      
      val <- val + log(MCMCpack::dwish(
        W = param$PREC, v = k, S = diag(rep(1, k)))) +
        sum(theta[as.integer(2:(k + 1))]) + 
        sum(log(2) + theta[-as.integer(1:(k + 1))] - 
              2 * log(1 + exp(theta[-as.integer(1:(k + 1))])))
      return(val)
    }
    initial <- function() {
      return(c(0, rep(log(1), k), rep(0, (k * (k - 1)/2))))
    }
    quit <- function() {
      return(invisible())
    }
    if (as.integer(R.version$major) > 3) {
      if (!length(theta)) 
        theta = initial()
    }
    else {
      if (is.null(theta)) {
        theta <- initial()
      }
    }
    val <- do.call(match.arg(cmd), args = list())
    return(val)
  }

inla.MLCAR.model <- function(...) INLA::inla.rgeneric.define(inla.rgeneric.MLCAR.model, ...)


```




#### R code for the BYM
A possible improvement on the modelistic side would be to give the multivariate BYM model a parametrisation allowing for a sparse precision matrix[@BYM2]


In the univariate framework, the model can be reparametrised as a joint model of the form $(z, u)$, such that  $\mathrm{Prec}[(\mathrm{vec}(z)^{\top}, \mathrm{vec}(u)^{\top})^{\top}]$ is indeed sparse [@BYM2]. In the multivariate framework, this idea can be easily applied to the joint field $(z, uM)$.

Therefore, for the time being we rely on the classic, dense parametrisation, which is however rather inefficient in computational terms.

Here we only show the R code to fit this model. The uniform prior of $\phi$ is another rather crude choice and more sophisticated priors ought to be implemented as well.

```{r mbym dense, eval = F}
inla.rgeneric.MBYM.dense <- 
  function (cmd = c("graph", "Q", "mu", "initial", "log.norm.const", 
                    "log.prior", "quit"), theta = NULL){
    envir <- parent.env(environment())
    if(!exists("cache.done", envir=envir)){
      starttime.scale <- Sys.time()
      #' Unscaled Laplacian matrix (marginal precision of u_1, u_2 ... u_k)
      L_unscaled <- Matrix::Diagonal(nrow(W), rowSums(W)) -  W
      L_unscaled_block <- kronecker(diag(1,k), L_unscaled)
      A_constr <- t(pracma::nullspace(as.matrix(L_unscaled_block)))
      scaleQ <- INLA:::inla.scale.model.internal(
        L_unscaled_block, constr = list(A = A_constr, e = rep(0, nrow(A_constr))))
      #' Block Laplacian, i.e. precision of U = I_k \otimes L
      n <- nrow(W)
      L <- scaleQ$Q[c(1:n), c(1:n)]
      Sigma.u <- MASS::ginv(as.matrix(L))
      if(PC == TRUE){
        #' Eigenvalues of the SCALED Laplacian, sufficient for trace and determinant entering the KLD
        L_eigen_scaled <- eigen(scaleQ$Q)$values
        #' PC prior on mixing parameter - definition should be fine.
        log.dpc.phi.bym <- INLA:::inla.pc.bym.phi(eigenvalues = L_eigen_scaled, 
                                                  marginal.variances = scaleQ$var,
                                                  rankdef = nrow(A_constr),
                                                  u = 0.5, alpha = 2/3)
        assign("log.dpc.phi.bym", log.dpc.phi.bym, envir = envir)
        
      }
      endtime.scale <- Sys.time()
      cat("Time needed for scaling Laplacian matrix: ",
          round(difftime(endtime.scale, starttime.scale), 3), " seconds \n")
      assign("L", L, envir = envir)
      assign("Sigma.u", Sigma.u, envir = envir)
      assign("cache.done", TRUE, envir = envir)
    }
    interpret.theta <- function() {
      alpha <-  1/(1 + exp(-theta[1L]))
      mprec <- sapply(theta[as.integer(2:(k + 1))], function(x) {
        exp(x)
      })
      corre <- sapply(theta[as.integer(-(1:(k + 1)))], function(x) {
        (2 * exp(x))/(1 + exp(x)) - 1
      })
      param <- c(alpha, mprec, corre)
      n <- (k - 1) * k/2
      M <- diag(1, k)
      M[lower.tri(M)] <- param[k + 2:(n + 1)]
      M[upper.tri(M)] <- t(M)[upper.tri(M)]
      st.dev <- 1/sqrt(param[2:(k + 1)])
      st.dev.mat <- matrix(st.dev, ncol = 1) %*% matrix(st.dev, 
                                                        nrow = 1)
      M <- M * st.dev.mat
      PREC <- solve(M)
      return(list(alpha = alpha, param = param, VACOV = M, 
                  PREC = PREC))
    }
    graph <- function() {
      PREC <- matrix(1, ncol = k, nrow = k)
      G <- kronecker(PREC, Matrix::Diagonal(nrow(W), 1) + 
                       W)
      return(G)
    }
    Q <- function() {
      param <- interpret.theta()
      #' Weighted average of ICAR and IID variables: variance is the sum of variances.
      #' Precision here defined as inverse variance. Not
      #' the best way to do it; still using sparse
      #' parametrisation requires a latent effect of length 2*np
      Sigma <- param$alpha * Sigma.u + (1-param$alpha)*diag(1, nrow(W))
      Q <- kronecker(param$PREC, solve(Sigma))
      return(Q)
    }
    mu <- function() {
      return(numeric(0))
    }
    log.norm.const <- function() {
      val <- numeric(0)
      return(val)
    }
    log.prior <- function() {
      param <- interpret.theta()
      if(PC == TRUE){
        #' PC prior implementation
        val <- log.dpc.phi.bym(param$phi)- theta[1L] - 2 * log(1 + exp(-theta[1L]))
      } else {
        #' Uniform prior
        val <- -theta[1L] - 2 * log(1 + exp(-theta[1L]))
      }
      #' Whishart prior on precision (inverse scale)
      val <- val + log(MCMCpack::dwish(W = param$PREC, v = k,
                                       S = diag(rep(1, k)))) +
        #' This for the change of variable
        #' (code from INLAMSM)
        sum(theta[as.integer(2:(k +  1))]) +
        sum(log(2) + theta[-as.integer(1:(k + 1))] - 2 * log(1 + exp(theta[-as.integer(1:(k + 1))])))
      return(val)
    }
    initial <- function() {
      if(!exists("init", envir = envir)){
        return(c(0, rep(0, k), rep(0, (k * (k - 1)/2))))
      } else{
        return(init)
      }
    }
    quit <- function() {
      return(invisible())
    }
    if (as.integer(R.version$major) > 3) {
      if (!length(theta)) 
        theta = initial()
    }
    else {
      if (is.null(theta)) {
        theta <- initial()
      }
    }
    val <- do.call(match.arg(cmd), args = list())
    return(val)
  }


```

Here we provide a preliminary code for the sparse parameterisation, still it needs more analysis and validation.
```{r mbym sparse, eval = F}
inla.rgeneric.MBYM.sparse <- 
  function (cmd = c("graph", "Q", "mu", "initial", "log.norm.const", 
                    "log.prior", "quit"), theta = NULL) {
    envir <- parent.env(environment())
    if(!exists("cache.done", envir=envir)){
      starttime.scale <- Sys.time()
      #' Unscaled Laplacian matrix (marginal precision of u_1, u_2 ... u_k)
      L_unscaled <- Matrix::Diagonal(nrow(W), rowSums(W)) -  W
      L_unscaled_block <- kronecker(diag(1,k), L_unscaled)
      A_constr <- t(pracma::nullspace(as.matrix(L_unscaled_block)))
      scaleQ <- INLA:::inla.scale.model.internal(
        L_unscaled_block, constr = list(A = A_constr, e = rep(0, nrow(A_constr))))
      #' Block Laplacian, i.e. precision of U = I_k \otimes L
      n <- nrow(W)
      L <- scaleQ$Q[c(1:n), c(1:n)]
      if(PC == TRUE){
        #' Eigenvalues of the SCALED Laplacian, sufficient for trace and determinant entering the KLD
        L_eigen_scaled <- eigen(scaleQ$Q)$values
        #' PC prior on mixing parameter - definition should be fine.
        log.dpc.phi.bym <- INLA:::inla.pc.bym.phi(eigenvalues = L_eigen_scaled, 
                                                  marginal.variances = scaleQ$var,
                                                  rankdef = nrow(A_constr),
                                                  u = 0.5, alpha = 2/3)
        assign("log.dpc.phi.bym", log.dpc.phi.bym, envir = envir)
        
      }
      endtime.scale <- Sys.time()
      cat("Time needed for scaling Laplacian matrix: ",
          round(difftime(endtime.scale, starttime.scale), 3), " seconds \n")
      assign("L", L, envir = envir)
      assign("cache.done", TRUE, envir = envir)
    }
    interpret.theta <- function() {
      phi <-  1/(1 + exp(-theta[1L]))
      mprec <- sapply(theta[as.integer(2:(k + 1))], function(x) {
        exp(x)
      })
      corre <- sapply(theta[as.integer(-(1:(k + 1)))], function(x) {
        (2 * exp(x))/(1 + exp(x)) - 1
      })
      param <- c(phi, mprec, corre)
      n <- (k - 1) * k/2
      M <- diag(1, k)
      M[lower.tri(M)] <- param[k + 2:(n + 1)]
      M[upper.tri(M)] <- t(M)[upper.tri(M)]
      st.dev <- 1/sqrt(param[2:(k + 1)])
      st.dev.mat <- matrix(st.dev, ncol = 1) %*% matrix(st.dev, 
                                                        nrow = 1)
      M <- M * st.dev.mat
      PREC <- solve(M)
      return(list(phi = phi, param = param, VACOV = M, 
                  PREC = PREC))
    }
    graph <- function() {
      BPrec <- matrix(1, ncol = 2*k, nrow = 2*k)
      G <- kronecker(BPrec, Matrix::Diagonal(nrow(W), 1) + 
                       W)
      return(G)
    }
    Q <- function() {
      param <- interpret.theta()
      Q11 <- 1/(1 - param$phi) * kronecker(param$PREC, Matrix::Diagonal(n = nrow(W), x  = 1))
      Q12 <- Q21 <- -sqrt(param$phi)/(1 - param$phi) * kronecker(param$PREC, Matrix::Diagonal(n = nrow(W), x  = 1))
      Q22 <- kronecker(param$PREC, 
                       ((param$phi/(1-param$phi))* Matrix::Diagonal(n = nrow(W), x = 1) + L))
      Q <- rbind(cbind(Q11, Q12), cbind(Q21, Q22))
      return(Q)
    }
    mu <- function() {
      return(numeric(0))
    }
    log.norm.const <- function() {
      val <- numeric(0)
      return(val)
    }
    
    log.prior <- function() {
      param <- interpret.theta()
      if(PC == TRUE){
        #' PC prior implementation
        val <-  log.dpc.phi.bym(param$phi) - theta[1L] - 2 * log(1 + exp(-theta[1L]))
      }else {
        #' Uniform prior
        val <- -theta[1L] - 2 * log(1 + exp(-theta[1L]))
      }
      #' Whishart prior on precision (inverse scale)
      val <- val + log(MCMCpack::dwish(W = param$PREC, v = k,
                                       S = diag(rep(1, k)))) +
        #' This for the change of variable
        #' (code from INLAMSM)
        sum(theta[as.integer(2:(k +  1))]) +
        sum(log(2) + theta[-as.integer(1:(k + 1))] - 2 * log(1 + exp(theta[-as.integer(1:(k + 1))])))
      return(val)
    }
    initial <- function() {
      if(!exists("init", envir = envir)){
        return(c(0, rep(0, k), rep(0, (k * (k - 1)/2))))
      } else{
        return(init)
      }
    }
    quit <- function() {
      return(invisible())
    }
    if (as.integer(R.version$major) > 3) {
      if (!length(theta)) 
        theta <- initial()
    }
    else {
      if (is.null(theta)) {
        theta <- initial()
      }
    }
    val <- do.call(match.arg(cmd), args = list())
    return(val)
  }

```


#### Addressing the potential issue of spatial confounding

The changes in $E[\beta|y]$ may suggest we are in presence of confounding bias. 
We adopt the most parsimonious approach to overcome this situation, i.e. removing spatial trends up to a certain order from all explanatory variables except for the distance from the closest AVC. In this case, interpreting a distance indicator from which spatial structure is removed becomes challenging. For all other covariates we follow the methodology of [@Urdangarin].

The fundamental decision regards the number of spatial trends to remove from each covariate. We try at removing as few spatial trends as possible in order to obtain a value of the Moran's standardised index less than $1.645$, which corresponds to the $95$-th percentile of the standard Normal distribution.

To do so, we remove the last $3$ eigenvectors from \texttt{ELI} covariate, $13$ eigenvectors from \texttt{PGR}, $19$ from \texttt{UIS}, $19$ from \texttt{ELL}, $6$ from \texttt{PDI} and $31$ from \texttt{ER}.

```{r deconfound, echo = FALSE}
deconfound <- function(X, Lapl = Lapl_con, n.eigen.out, rescale = T){
  if(!is.matrix(X)) {
    as.vectorX <- TRUE
    X <- as.matrix(X)
  } else {
      as.vectorX <- FALSE
      }
  V <- eigen(Lapl)$vectors
  rk <- Matrix::rankMatrix(Lapl)
  coef <- solve(V, X)
  eigen.in <- c(1:(rk-n.eigen.out), c((rk+1):ncol(V)))
  X_nosp <- V[, eigen.in] %*% coef[eigen.in, ]
  if(rescale) X_nosp <- scale(X_nosp)
  if(as.vectorX) X_nosp <- as.vector(X_nosp)
  return(X_nosp)
}

dd_list_nosp_fn <- function(n.eigen.out){
  res  <- list (
  Intercept = dd_list$Intercept,
  N_ACC = dd_list$N_ACC,
  TEP_th = dd_list$TEP_th,
  ELI = matrix(c(deconfound(dd_con$ELI, n.eigen.out=n.eigen.out), rep(NA, 3*n), 
                deconfound(dd_con$ELI, n.eigen.out=n.eigen.out), rep(NA, 3*n),  
                deconfound(dd_con$ELI, n.eigen.out=n.eigen.out)), nrow = 3*n,
              ncol = 3, byrow = FALSE),
  PGR = matrix(c(deconfound(dd_con$PGR, n.eigen.out=n.eigen.out), rep(NA, 3*n), 
                deconfound(dd_con$PGR, n.eigen.out=n.eigen.out), rep(NA, 3*n),  
                deconfound(dd_con$PGR, n.eigen.out=n.eigen.out)), nrow = 3*n,
              ncol = 3, byrow = FALSE),
  UIS =  matrix(c(deconfound(dd_con$UIS, n.eigen.out=n.eigen.out), rep(NA, 3*n), 
                deconfound(dd_con$UIS, n.eigen.out=n.eigen.out), rep(NA, 3*n),  
                deconfound(dd_con$UIS, n.eigen.out=n.eigen.out)), nrow = 3*n,
              ncol = 3, byrow = FALSE),
  ELL = matrix(c(deconfound(dd_con$ELL, n.eigen.out=n.eigen.out), rep(NA, 3*n), 
                deconfound(dd_con$ELL, n.eigen.out=n.eigen.out), rep(NA, 3*n),  
                deconfound(dd_con$ELL, n.eigen.out=n.eigen.out)), nrow = 3*n,
              ncol = 3, byrow = FALSE),
  PDI = matrix(c(deconfound(dd_con$PDI, n.eigen.out=n.eigen.out), rep(NA, 3*n), 
                deconfound(dd_con$PDI, n.eigen.out=n.eigen.out), rep(NA, 3*n),  
                deconfound(dd_con$PDI, n.eigen.out=n.eigen.out)), nrow = 3*n,
              ncol = 3, byrow = FALSE),
  ER = matrix(c(deconfound(dd_con$ER, n.eigen.out=n.eigen.out), rep(NA, 3*n), 
                deconfound(dd_con$ER, n.eigen.out=n.eigen.out), rep(NA, 3*n),  
                deconfound(dd_con$ER, n.eigen.out=n.eigen.out)), nrow = 3*n,
              ncol = 3, byrow = FALSE),
  ID = dd_list$ID,
  nn = dd_list$nn)
  return(res)
}

dd_list_nosp <- list (
  Intercept = dd_list$Intercept,
  N_ACC = dd_list$N_ACC,
  TEP_th = dd_list$TEP_th,
  ELI = matrix(c(deconfound(dd_con$ELI, n.eigen.out=3), rep(NA, 3*n), 
                deconfound(dd_con$ELI, n.eigen.out=3), rep(NA, 3*n),  
                deconfound(dd_con$ELI, n.eigen.out=3)), nrow = 3*n,
              ncol = 3, byrow = FALSE),
  PGR = matrix(c(deconfound(dd_con$PGR, n.eigen.out=13), rep(NA, 3*n), 
                deconfound(dd_con$PGR, n.eigen.out=13), rep(NA, 3*n),  
                deconfound(dd_con$PGR, n.eigen.out=13)), nrow = 3*n,
              ncol = 3, byrow = FALSE),
  UIS =  matrix(c(deconfound(dd_con$UIS, n.eigen.out=19), rep(NA, 3*n), 
                deconfound(dd_con$UIS, n.eigen.out=19), rep(NA, 3*n),  
                deconfound(dd_con$UIS, n.eigen.out=19)), nrow = 3*n,
              ncol = 3, byrow = FALSE),
  ELL = matrix(c(deconfound(dd_con$ELL, n.eigen.out=19), rep(NA, 3*n), 
                deconfound(dd_con$ELL, n.eigen.out=19), rep(NA, 3*n),  
                deconfound(dd_con$ELL, n.eigen.out=19)), nrow = 3*n,
              ncol = 3, byrow = FALSE),
  PDI = matrix(c(deconfound(dd_con$PDI, n.eigen.out=6), rep(NA, 3*n), 
                deconfound(dd_con$PDI, n.eigen.out=6), rep(NA, 3*n),  
                deconfound(dd_con$PDI, n.eigen.out=6)), nrow = 3*n,
              ncol = 3, byrow = FALSE),
  ER = matrix(c(deconfound(dd_con$ER, n.eigen.out=31), rep(NA, 3*n), 
                deconfound(dd_con$ER, n.eigen.out=31), rep(NA, 3*n),  
                deconfound(dd_con$ER, n.eigen.out=31)), nrow = 3*n,
              ncol = 3, byrow = FALSE),
  ID = dd_list$ID,
  nn = dd_list$nn) 

```

Then we fit the BYM model to the deconfounded data
```{r MBYM nosp, echo = FALSE, message = FALSE, output = FALSE}


cav_MBYM_inla_spatplus <- inla(
  N_ACC ~ 0 + Intercept +TEP_th + ELI + PGR + UIS + ELL + PDI + ER+ 
    f(ID, model = inla.MBYM.dense(k = 3, W = W_con, 
                                  PC = FALSE),
      extraconstr = list(A = A_constr, e = rep(0, 3))) ,
  offset = log(nn),
  family = rep("poisson", 3), data =dd_list_nosp,
  num.threads = 1, control.compute = list(internal.opt = F, cpo = T, waic = T, config = T), 
  verbose = T)

```

Resulting estimates for covariate effects are summarised:
```{r betas leroux spatplus, echo = FALSE, results = "asis"}
betas.mbym.inla.1  <- cav_MBYM_inla_spatplus$summary.fixed %>% #[-1,] 
  dplyr::select(.data$mean, .data$sd)  %>%
  dplyr::mutate( year = rep(c(2021, 2022, 2023), 8)) %>% 
  dplyr::mutate(Effect = rep(c("Intercept", "TEP", rownames(summary(cav_glm_21)$coefficients)[-c(1,2)]),each=3)) %>% 
  dplyr::relocate(.data$Effect, .before = 1) %>%  
  dplyr::rename(Mean = .data$mean, Sd = .data$sd) %>% 
  tidyr::pivot_wider( 
    id_cols = .data$Effect,            
    names_from = .data$year,                
    values_from = c(.data$Mean, .data$Sd))

#betas.mlcar.inla1 <- dplyr::bind_rows(
#  data.frame(Effect = "Intercept", 
#             Mean_2021 = cav_MLCAR_inla_spatplus$summary.fixed$mean[1],
#             Sd_2021 = cav_MLCAR_inla_spatplus$summary.fixed$sd[1],
#             Mean_2022 = cav_MLCAR_inla_spatplus$summary.fixed$mean[1],
#             Sd_2022 = cav_MLCAR_inla_spatplus$summary.fixed$sd[1],
#             Mean_2023 = cav_MLCAR_inla_spatplus$summary.fixed$mean[1],
#             Sd_2023 = cav_MLCAR_inla_spatplus$summary.fixed$sd[1]),
#  betas.mlcar.inla.noint1) 

print(xtable::xtable(betas.mbym.inla.1, num.digits = 3), include.rowname = FALSE)

```

The most notable change is perhaps the shrinkage in the effect of employment rate in 2021 and 2023. This being said, the interpretation of the posteriors of $\beta$ is overall consistent with the base MLCAR model. 










